
----------------------------------------------------------------------------------------------------------------------------------------

0)normally, git pull in heroku, heroku gives an ip and expose its one port then run the app run command.
and maybe if want run a scheduled job to pull from github and run the app again 

1)but my ci/cd and cloud,
travis ci, whenever pushed to github, automatically builds the image and hosts.(doesnt wait for scheduled job)
and the gcp pull the image and restarts only those specific docker containers using those images

(
  2)if scheduled job with gcp instead of ci/cd and containers then just like heroku, but gcp gives better features like vnet, firewall, image/blob storage etc.
  3)also containerization needed as every service not dependent on another and each service can be updated/scaled independently
            (else different services run like different apps/processes on host-port if not using containers) 
  as ci/cd listens to that containers entry folder in github
  4)if scheduled job with docker in gcp instead of ci/cd then gcp will have to pull and build image everytime as running on containers not natively
   (means gcp is running the .sh commands job instead of ci/cd online shells)
)




----------------------------------------------------------------------------------------------------------------------------------------


Done with Docker, CI/CD(just shell scripts online shell) and Github(Dockerhub also same) at this point. only Cloud and TF left.





If you're using Azure CLI over a proxy server that uses self-signed certificates,
 the Python requests library used by the Azure CLI might cause the following error: SSLError("bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)",). To address this error, 
 set the environment variable REQUESTS_CA_BUNDLE to the path of CA bundle certificate file in PEM format.


export REQUESTS_CA_BUNDLE=/Users/abhinav.bhattacharje/mern/infrastructure-as-code-talk-docker-terraform/infrastructure-as-code-talk/sinatra-backend/proxy-cert-2.pem 


azure cloud - 
Azure Cloud Shell requires a storage account with Azure file share to persist files(like volumes on docker, else all file data and state deleted). 
Select an option below to mount a storage account
subscription - 54001000 abhinavbhattacharjee
snow subscription login using sso
resource group - testvm

Subscription: 54001000 abhinavbhattacharjee
Resource group: 88a798b0-9501-4368-ba51-65960c63f5ea/resourcegroups/cloud-shell-storage-centralindia
Storage account: csg10032002b8b2b06f
File share: cs-abhinav-bhattacharje-servicenow-com-10032002b8b2b06f
Region: Central India

>az account show
>az group list
>az group show --resource-group testvm
>az vm image list //all the default vm images(like packer images) for unix/windows etc on azure hub(like docker base images)
>az vm list
>az vm show -n testvm1 --resource-group testvm
>az vm create -n MyVm -g MyResourceGroup --image Ubuntu2204

just create your own azure , aws, and gcp account and use in personal laptop



-------------------------------------------------------------------------------

AWS ECS is just a logical grouping (cluster) of EC2 instances, and all the EC2 instances part of an ECS act as Docker host i.e. 
ECS can send command to launch a container on them (EC2). 


Docker is a utility you can install on our machine, which makes it a Docker host, and on this host you can run a vms which creates containers (same as virtual machines - 
but much more light-weight). To sum up, ECS is just about clustering of EC2 instances, and uses Docker to instantiate many containers/instances/virtual
 machines on these (EC2) hosts.


EC2 allows you to launch individual instances which you can use for pretty much whatever you like. ECS is a container service,
 which means it will launch instances that will be ready to launch container applications
 Elastic Container Service (ECS) is a Docker container orchestration service.

You can ask it to run one or more Docker images, either as an auto-scaling capable "Service" or as an ad-hoc "Task".
The services and tasks run on a "Cluster". Originally, a Cluster was a group of one or more pre-configured EC2 servers running ECS Cluster Agent. The Cluster Agent would schedule the containers on the EC2 server. These EC2 servers show up in your EC2 Instances list and are charged at regular EC2 per-minute costs - You can even SSH onto them like any normal EC2 server. If you wanted more capacity to run more Services or Tasks, or if you wanted resilience against EC2 failure, then you would more EC2 servers.
Around November 2017, AWS added ECS Fargate. Now a Cluster can run "serverless" without provisioning EC2 nodes(means provisioning EC2 handled by amazon instead of us based on ram/cpu).
 You simply define the amount of CPU and memory your Task or Service requires to operate, meaning that you just pay for the CPU and memory time rather than the EC2.

ECS is a container orchestrator just like Kubernetes or Docker swarm, EC2 is an Amazon Elastic Compute Cloud platform for Creating Virtual Machines.
 ECS allows you to run containers on either serverless environments (Fargate)(amazon handles the ec2s, we manage the containers) where you don't have to run any VM or in a non managed environments 
 where you host the containers on EC2 instances.(we manage both ec2s and containers on them)

https://stackoverflow.com/questions/40575584/what-is-the-difference-between-amazon-ecs-and-amazon-ec2




--------


https://www.slideshare.net/brikis98/infrastructure-as-code-running-microservices-on-aws-using-docker-terraform-and-ecs?ref=http://www.ybrikman.com/writing/2016/03/31/infrastructure-as-code-microservices-aws-docker-terraform-ecs/#152
https://github.com/abhinav455/abhinav455-infrastructure-as-code-talk-docker-terraform-aws.git

actual tf.main :-


module "rails_frontend" {
  source = "./ecs-service"

  name = "rails-frontend"
  ecs_cluster_id = "${module.ecs_cluster.ecs_cluster_id}"

  image = "${var.rails_frontend_image}"
  image_version = "${var.rails_frontend_version}"
  cpu = 1024
  memory = 768
  desired_count = 2

  container_port = "${var.rails_frontend_port}"
  host_port = "${var.rails_frontend_port}"
  elb_name = "${module.rails_frontend_elb.elb_name}"

  # Provide the URL of the sinatra-backend ELB using the same environment variable name and format as docker-compose
  num_env_vars = 2
  env_vars = "${map(
    "RACK_ENV", "production",
    "SINATRA_BACKEND_PORT", "tcp://${module.sinatra_backend_elb.elb_dns_name}"
  )}"
}
-------


inside tf module .main :-

# CREATE AN ECS SERVICE TO RUN A LONG-RUNNING ECS TASK
# We also associate the ECS Service with an ELB, which can distribute traffic across the ECS Tasks.
# --------------------------------------------------

resource "aws_ecs_service" "service" {
  name = "${var.name}"
  cluster = "${var.ecs_cluster_id}"
  task_definition = "${aws_ecs_task_definition.task.arn}"
  desired_count = "${var.desired_count}"
  iam_role = "${aws_iam_role.ecs_service_role.arn}"

  deployment_minimum_healthy_percent = "${var.deployment_minimum_healthy_percent}"
  deployment_maximum_percent = "${var.deployment_maximum_percent}"

  load_balancer {
    elb_name = "${var.elb_name}"
    container_name = "${var.name}"
    container_port = "${var.container_port}"
  }

  depends_on = ["aws_iam_role_policy.ecs_service_policy"]
}

# -----------------------------------------------------
# CREATE AN ECS TASK TO RUN A DOCKER CONTAINER
# -------------------------------------------------------

resource "aws_ecs_task_definition" "task" {
  family = "${var.name}"
  container_definitions = <<EOF
[
  {
    "name": "${var.name}",
    "image": "${var.image}:${var.image_version}",
    "cpu": ${var.cpu},
    "memory": ${var.memory},
    "essential": true,
    "portMappings": [
      {
        "containerPort": ${var.container_port},
        "hostPort": ${var.host_port},
        "protocol": "tcp"
      }
    ],
    "environment": [${join(",", data.template_file.env_vars.*.rendered)}]
  }
]
EOF
}
--------


In terraform providers
---deploy
1) Ecs_cluster manages each EC2 Instance that will run in the ECS Cluster

    aws_autoscaling_group -   size = 6
2) Ecs_instance/aws_launch_configuration is like a aws host vm( based on customised packer like image launch configuration we mention in ecs_instance)
         ecs_instance - image_id = "${data.aws_ami.ecs.id}" 
         data "aws_ami" "ecs" {  values = ["amzn-ami-*-amazon-ecs-optimized"],}   
         ecs_cluster manages this like t2.micro
3) inside Ecs_instance(host vm), we create Ecs_service(docker vm)  (they have elb load balacer configured to talk to other docker/host vm)
         //doesnt need any image as the standards docker just need config
          all ecs_service managed by cluster ecs_cluster
4) inside Ecs_service(docker vm) we create aws_ecs_task_definition(docker container/task)  (based on customised docker image we mention in aws_ecs_task_definition)

  2.2) Ec2 is like a aws host vm thus needs ec2 packer image(like aws_launch_configuration like t2 micro)


-----redeploy
1)here whenever the docker image needs to be build again and put on dockerhub, 
2)there are already running ecs cluster, ecs instances/host vm, running ecs services/docker vm
  and inside that running containers. we just need to remove and again run/ compose-up the containers on aws using
  ecs_cluster commands/ new terraform file mayb/ansible(configure)/ just for containers not provision  whole vms, clusters, load balancers etc.
3)but just for easy way they told us to again terraform init,plan, apply to reprovision all infra and 
   resources, will obviously then use need docker built images from the hub

   3.2) all these steps can also automate with shell jenkins ci/cd scipts .yaml .sh
export AWS_ACCESS_KEY_ID=...
export AWS_SECRET_ACCESS_KEY=...
vars.tf - main vars.tf tweak if want docker images and their ports just, 
          the security, instance count/sizes etc in other modules
>terraform init,plan,apply 
init initializes the provider plugins and the modules, how our new infra looks like
plan compares with old infra using old .tfstate file(stored in secure s3/blob), makes a plan how to create/update old infra
    to see any changes that are required for your infrastructure
apply makes the changes according to the plan


>terraform plan -out tfplan
      data.aws_subnet.default[2]: Read complete after 0s [id=subnet-0845d5267b4ec1b49]
      data.aws_subnet.default[1]: Read complete after 0s [id=subnet-0d85a1d6e98337810]
      module.rails_frontend.data.template_file.env_vars[0]: Reading...
      module.rails_frontend.data.template_file.env_vars[0]: Read complete after 0s [id=187c415a5554e3f8f89a9a778ddcd5445a4df052ae28e1b501a6250a79b1220d]
     
      Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
        + create
      <= read (data resources)
      Terraform will perform the following actions:
        # module.ecs_cluster.aws_autoscaling_group.ecs_cluster_instances will be created

        # module.sinatra_backend_elb.aws_security_group_rule.all_outbound_all will be created
        + resource "aws_security_group_rule" "all_outbound_all" {
            + cidr_blocks              = [
                + "0.0.0.0/0",
              ]
            + from_port                = 0
            + id                       = (known after apply)
            + protocol                 = "-1"
            + security_group_id        = (known after apply)
            + security_group_rule_id   = (known after apply)
            + self                     = false
            + source_security_group_id = (known after apply)
            + to_port                  = 0
            + type                     = "egress"
          }

      Plan: 25 to add, 0 to change, 0 to destroy.

      Changes to Outputs:
        + rails_frontend_url  = (known after apply)
        + sinatra_backend_url = (known after apply)
      ╷
>terraform apply "./tfplan"


Done with Docker, CI/CD(just shell scripts online shell) and Github(Dockerhub also same) at this point. 
Cloud and TF also done(just shell scripts to configure and provision infra, instances, networks for deploy etc)
    
----------------------------------------------------------------------------------------------------------------------------------------


hypervisor vm for each application before vm.
application in a  vm will be restricted/dependent on the os and libraries of the os(os+libraries+app)

thus instead of many vm/os for each app, we have one single docker server virtual vm(thus same OS across all apps) running each app in a isolated container(libraries+app) with all
libraries installed

yaml,xml, json data serializer language
yaml files for containers and github actions ci/cd

-------
Building for production  //while dev build and run at the same time
>npm run build //>npx react-scripts build

    File sizes after gzip:

      278.43 kB  build\static\js\main.6681b3c8.js
      28.45 kB   build\static\css\main.feaf2411.css

    The project was built assuming it is hosted at /.
    You can control this with the homepage field in your package.json.

    The build folder is ready to be deployed.
    You may serve it with a static server:

      npm install -g serve
      serve -s build

    Find out more about deployment here:

      https://cra.link/deployment

in production, just add the built bundle.js and bundle.css code build using webpack etc as just serve using a static server
no need to do npm run start as does both 1)build again and again before running in prod and 2)servers using react cra server

after build we dont need cra server(although can use), can use vite/nginx for static serve

building better in prod as optimised build and can use any server and dont need to build and run at same time but one after another


CMD ["npm", "run", "server"] #not using nginx but our own cra static server to serve 

for nginx, in same dockerfile add -  
# production environment
FROM nginx:stable-alpine
COPY --from=build /app/build /usr/share/nginx/html
COPY nginx/nginx.conf /etc/nginx/conf.d/default.conf
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]

Here, we take advantage of the multistage build pattern to create a temporary image used for building the artifact – the production-ready React static files 
– that is then copied over to the production image. The temporary build image is discarded along with the original files and folders associated with the image. 
This produces a lean, production-ready image.


>docker build -f Dockerfile.prod -t sample:prod .
>docker run -it --rm -p 1337:80 sample:prod

---
in docker-compose, not necessary to use an already build image, can build from dokcerfile and run at same time(anyways iterative build from our built image in docker-compose only)
(instead of building from our built image, building from alpine base image)

version: '3.7'

services:

  sample-prod:
    container_name: sample-prod
    build:
      dockerfile: Dockerfile.dev
      context: ./    #already built image in local is there in our context local thus doesnt build again     #building again from dockerfile if not already prebuilt image
    ports:
      - '1337:80'

>docker-compose -f docker-compose.prod.yml up -d --build      

>docker run imagename ls //runs containers and lists all filesystem in it like we are in the terminal of container -it -tty like, but background only -d
>docker-compose build //buid image from dockerfile mentioned in docker-compose file 
>docker-compose down //delete the built image

>docker-compose up   //automatially detects the built image
>docker-compose run server sh  //goes into container shell

node_modules in windows wont work in dockercontainer linux need to reinstall
----------

dotenv module loads the variables from .env file into process.env module
require("dotenv").config();  //console.log(process.env.MY_NAME)

------
depends_on:    #dont want to run container client before server 
  - server

------------------------

ci/cd (.sh .yaml) 
1) push to github, github action/jenkins/ci-cd listens
2)github action build image from githhub and push to dockerhub
3)github action make gcp pull from dockerhub by running docker-compose up again on vm
   / or provision all infa again using terraform init/plan/apply


1)Jenkins if run in cloud then can run in local also(can connect to internet from local like push dockerhub/ terraform on aws) 
and pull from git-build-push to dockerhub, pull from dockerhub-push to gcp with gcp tokens and api

2)thus jenkins allows using local also, but travis only allows on cloud(local is paid) but for production
need cloud only as we dont maintain our server. 
(or can run jenkins locally on any gcp vm like servicenow runs its own local jenkins controller on vm)


normal actions build/imoprt from marketplace OR pipelines for complex steps

3)shipping apps to production just means ship our code files and a command to run them and standardised infra (done in dockerfile)
  and the infra we get from containers/vm


-------------------------------------------------------------------------------------------------------------


















--------------------------------------------------------


jenkins stage(many steps) like describe(many tests) in jest/playwright/cypress

build - a group of jobs that run in sequence. 
stage - a group of jobs that run in parallel as part of a sequential build process composed of multiple stages
job - an automated process that clones your repository into a virtual environment and then carries out a series of phases such as compiling your code, running tests, etc. 
A job fails, if the return code of the script phase is non-zero. 
   if all code run successfulyy then job passed like in >docker run -- --coverage run successfully, coverage for mocha pass arguments
phase - the sequential steps of a job. For example, the install phase, comes before the script phase, which comes before the optional deploy phase.


Travis CI doesnt use github ssh like local jenkins, but uses oauth access tokens from github 
resource server to pull the repo code/files.
thus travis better for github as oauth also configure once for online travis server

Travis CI’s system clones a repository, from which the build is triggered, to the build environment. 
The build environment is an isolated virtual machine or an LXD container, which gets terminated as soon as the build finishes.





----------------
You need to add .travis.yml configuration file to root
directory of your repository. If a .travis.yml is not in your
repository or is not a valid YAML, Travis CI will ignore.it

Once you've added .travis.yml to your repo, all you
need to do now is commit your changes and push
them to the repository(means initial listening step on push already configured on travis for the particular repo/ or github webhook configure to call travis)

configure when to trigger build on travis on github commit, pull request, new branch
via ui only.
but what to do when build starts add to travis.yml file in your repo

services:
  - docker   #runs $sudo systemctl start docker in job shell
$ git clone --depth=50 --branch=main https://github.com/abhinav455/e-commerce-server.git abhinav455/e-commerce-server  #default

IMP----
   local->github(push files)->travis(create docker image with files / github-dockerhub giving their own connect ci/cd for these tasks)->dockerhub->travis/terraform  
      ->gcp(travis restart gcp docker-compose(but travis is a local/cloud shell thus will use gcp api/cli inside its shell) / terraform pull image and provision vms and containers again)


test script in ci/cd to make sure the application built via ci/cd is running properly
 >npm test or  >npm run jest// we are using mocha(nyc-mocha on top)
 "scripts":
    "test": "DEBUG=true nyc mocha ./test/**/**/*.test.js --exit --inspect=0.0.0.0:9292 --timeout=12000
         

mostly all .sh files in our repo as the files run by the ci/cd during 
  deployment process and not directly used by our app
 script: bash docker_push
  #docker is a script in repo containenint 
  #!/bin/bash ; echo "$DOCKER_PASSWORD" | docker login -u "$DOCKER_USERNAME" --password-stdin ; docker push USER/REPO  


  "devDependencies": { /*ENV NODE_ENV=production in Dockerfile*/  //while running npm install it checks NODE_ENV
    "nodemon": "^3.1.3"
  }

-----------------------------------------------------------------------------------------




 --------------------

testing framework can be of 2 types - 
1) unit testing on node nev - import any library and its fns and check the expected output uses the same enviroment where testing server is running. 
         you dont need browser/page fns and libraries for these
2) end to end testing opens a browser and page instance and navigates to a url(public or localhost url(frontend/backend server hosting bundlejs and running on another terminal/process))
        the web server running another process/environment


---------------
docker run bhattacharjeeabhinav/ecommerceapp:server npm run test -- --coverage   //run npm start(already mentioned in dockerfile is ignored) and npm test(command passed as params) is run
                                                                                         //-- --coverage so that runs and exits the container
 
IMP - 1)volume is mounted before CMD is run in docker and docker-compose (even if already built file and built only till before CMD)
      2)thus rest everything use RUN command as used to specify steps during build process. CMD to specify default cmd to run when docker container is run, not a part of build process, 
          runs after new mount. ENTRYPOINT same as CMD, just that if params in docker-run cmd, then entrypoint not ignored like cmd, but first run and then the params passed cmd run.


----------
#instead of password file, add env variable in travis to get DOCKER_PASSWORD
using STDIN prevets password from ending up in shell's history. pipe the stdin docker hub password
  #!/bin/bash ; echo "$DOCKER_PASSWORD" | docker login -u "$DOCKER_ID" --password-stdin ; docker push USER/REPO

 script step tells steps of travis that need to pass for job to be successful

--------

env file not pushing to github. so in gcp the docker container wont have env file so app wont work.
1) just like travis, create env variables in all cloud resources like google cloud/github(settings->secrets if used github actions)/travis
2) push encrypted env vars by openssl and decrypt those from file in gcp container at runtime(in node code only decrypt using openssl)

A)rsa - encrypt and decrypt ; public and private key factoring primes, asymmetric (p,q,t) (e,d,n)
B)md5/sha/bcrypt(salt and exponent) - encrypt using irreversible hash; block bitshift and xor (used to encrypt aes session symmetric key transfer during ssl)
C)aes - encryt and decrypt, symmetric; same key for both encryption and decryption, 4x4 grid transpose matrix multiply, then the 2nd matrix is key, A*A=I means A=A^(-1), A=key is the root 
                                          (xor, substitute, permute rowns and columns, add round key; key expanded and changed for every round), decrypt also same


echo 'test'> text.txt //write
echo 'test'>> text.txt //append

> \"Program Files"\Git\usr\bin\openssl 
>openssl enc -aes256 -e -in .env -out .enc -k password1 -md sha256
>openssl enc -aes256 -d -in .enc -out .env -k password1 -md sha256  //message digest sha256 or md5

decrypt in gcp/travis ci, just need to create one env var i.e our aes key

bgz8xe95dTeftkB - pass


-------------------------------------

react tests, >react-scripts test, its own testing framework of cra/react like , 
dont need jest/mocha other testing libraries for node as react itself good library for test also(uses jest only under) 
import App from "../App";  
   //doing unit testing thus need to import the files as running test in same node environment
      //and not e2e test on browser using browser/page instances and navigating to localhost page hosted by another server process

Basically, jest runs on Node.js environment, so it uses modules following the CommonJS. If you want to use axios up to 1.x.x, you have to transpile the JavaScript module from ECMAScript type to CommonJS type. 
Jest ignores /node_modules/ directory to transform basically.
runs our test and also react prebuilt tests for all files and components

>npm install serve  //server production built react static files
>npm run build
 "build": "react-scripts build"
 "server" : "serve -s build"
>npm run server 

make your own express app to server built index.html and static files js and css in build folder

By default, serve will listen on 0.0.0.0:3000 and serve the
current working directory on that address

-----------------------------------------


















---------------------------------------------------------------------------------------


GCP

Gcp sdk OR cli for ci/cd .sh scripts to provision and configure vms vnets policies elbs etc. but also to ssh into vms and run containers. Or without ssh use thier ecs
cli, rest api, sdk
npm install --save @google-cloud/pubsub

C:\Users\AbhinavBhattacharjee\AppData\Local\Google\Cloud SDK\google-cloud-sdk\bin\
>gcloud auth list
>gcloud config list
>gcloud app versions list

App Engine is a fully managed, serverless platform for developing and hosting web applications at scale. 
You can choose from several popular languages, libraries, and frameworks to develop your apps, and then
 let App Engine take care of provisioning servers and scaling your app instances based on demand.

thus serverless main feature. other way is to use Create a client-server application on Compute Engine 2 vms
connect to the app using client-vm-public-ip:port normally

If you want to terminate this service you can run the following command during an ssh session logged in to the client VM:
kill -9 $(sudo lsof -i :80 | grep \
    main | awk '{print $2}' )


create a service account on gcp to connect to cloud and use its password in travis.
service account - eshop

like a private key, serviceAccount.json. //json includes project, user other info fields and private key field also
when we connect to cloud to a project, the cloud looks inside the project to see if this account has the authorzation policy
same like .env .enc. use openssl aes to push encrypted serviceAccount.json.enc to github. and then delete the file from your system also. for both client and server repos
 and add SERVICE_KEY env in travis


-------
eshop-431807
Travis will deploy the container to gcp and new app instance of gcp will be created

travis.yml
Need to run gcp interactive commands in a non interactive way travis - CLOUDSDK_CORE_DISABLE_PROMPTS=1 (accepts all default interactive)

curl https://sdk.cloud.google.com(gets all the bash commands to install gcp script.sh) | bash (pipe it to bash, bash will run those commands)
//#!/bin/bash URL=https://dl.google.com/dl/cloudsdk/channels/rapid/install_google_cloud_sdk.bash function download { scratch="$(mktemp -d -t tmp.XXXXXXXXXX)" || exit script_file="$scratch/install_google_cloud_sdk.bash" echo "Downloading Google Cloud SDK install script: $URL" curl -# "$URL" > "$script_file" || exit chmod 775 "$script_file" echo "Running install script from: $script_file" "$script_file" "$@" } download "$@"

zsh for mac, bash for linux , sh for linux(bash is sh+more features) !#/bin/bash

PATH="dir:$PATH", you are effectively adding dir to the beginning of the PATH variable, each path  separated by :
if [-f path_to_file];  then  . 'path_to_file'; fi  
        //-f checks if the path exists and is a regular file 
        // . in bash is shorthand for source command, runs script in current shell env (any env or fn defined in it will be present in current shell)
        //-d checks if the directory exists
        //rm -rf; rm means delete, -r means recursively delete subfolders and files, -f means force delete all even write protected and dont ask for user confirmation
eval $command  //evaluates/run in another shell maybe whatever
 echo "${array[@]}" #return //expands all elements in array as separate arguments
  $() syntax is used for command substitution and not for array indexing ${array[$i]}
    local gcloud="$(gcloud app versions list)"



IMP - in .travis.yml, current directory is the directory in which we do git clone(we do gitclone and cd into the cloned directory first, travis does implicit)
---

  - gcloud auth activate-service-account --key-file serviceAccount.json  #instead of logging in via oauth our owner account

  # - gcloud deploy app #deploys the current directory all files
    #(uses the app.yml file to know which command to run and env to run on -node/python etc)

- bash ./deploy.sh  //remove old deployments else - gcloud deploy app , everytime creates a new app instance on the container/vm/ec2/compute instance
 1)get list of all deployment options
    >gcloud app versions list
 2)remove older deployments
    >gcloud -q app versions delete $version
 3)deploy the new app 
    >gcloud app deploy --stop-previous-version
                #deploys using the app.yml file in the current directory like docker-compose.yml, take all current directory files 
                #stop current not fully deployed also
                #it sees the docker-compose file thus runs the env as containers looking at it
------

#################
The dispatch.yaml allows you to override routing rules. 
You can use the dispatch.yaml to send incoming requests to a specific service (formerly known as modules) based on the path or hostname in the URL.
>gcloud app deploy dispatch.yaml

dispatch:
  # Default service serves the typical web resources and all static resources.
  - url: "*/favicon.ico"
    service: default
  - url: "*/mobile/*"
    service: mobile-frontend  

cron.yaml like app.yaml
A cron job makes a scheduled HTTP GET request to the specified endpoint in the same app where the cron job is configured. The handler for that endpoint executes the logic when it is called.
The App Engine Cron Service cannot be used to call web endpoints outside the App Engine host app. It cannot be used to call App Engine endpoints from other apps besides the host app.
cron:
  - description: "daily summary job"
    url: /tasks/summary
    schedule: every 24 hours
##################


app.use(express.static(path.join(__dirname, "/build"))); //express should understand the path //middleware to serve other static files
 server: node app.js ////"serve -s build"
update stripe secret key but need to updated env vars also thus let it be


app.yaml
https://cloud.google.com/appengine/docs/flexible/reference/app-yaml?tab=node.js#top


resources:
  cpu: 2
  memory_gb: 2.3
  disk_size_gb: 10
  volumes:
  - name: ramdisk1
    volume_type: tmpfs
    size_gb: 0.5
automatic_scaling:
  min_num_instances: 1
  max_num_instances: 15
  cool_down_period_sec: 180
  cpu_utilization:
    target_utilization: 0.6
  target_concurrent_requests: 100


>gcloud app logs tail -s default
enable compute and app engine apis, app engine flexible api and service management api(for service account) for the project

-q flag to run in quiet mode for non interactive ci/cd job

>gcloud config set compute/zone us-west2-a //uses compute engine apis thus need to set it
https://console.cloud.google.com/apis/api/compute.googleapis.com/metrics?project=eshop-431807

gcloud cli after deploying on travis gives a public url for the app, go to the url and see if server app is up
now need to take this url and give to client react app to connect
server public url - https://eshop-431807.uc.r.appspot.com/
client public url - https://client-dot-eshop-431807.uc.r.appspot.com

>gcloud app instances ssh "aef-default-20240808t163658-bn7v" --service "default" --version "20240808t163658" --project "eshop-431807"
>gloud app browse -s client  //or default

-----
//app engine uses the docker-compose.yml by default.
//app engine maintins its own docker built image and built context. but in docker-compose dont at built context as will not have context the first time.
// just add pull from hub for gcp which the travis ci pushed else purpose defeated
uses dockerfile and build again on gcp everytime only. why not pulling from dockerhub?


IMP : the service: name app.yml unique. thus always knows that same service being deployed again, do versioning and give same url and same docker context.


IMPIMP - not like 2 containers running on 2 vms, but 2 containers running on same vm OR 2vm but on ecs like thus acts like one vm only
      but mayb like 2 different vms only, one doesnt know another. but since we have public url client can talk to server. 
      
      we are not using docker-compose(just using docker compose locally) but still both containers/services running on different containers on different
      vms able to talk to each other as we have public url and cors enabled all on server
  
  I believe not. Flexible runtime is for auto-scaling a single Dockerfile-based app
  It is not possible at this time to use docker-compose to have multiple application containers within a single App Engine instance. This does seem however to be by design.
  means for each container/service serverless application creating an instance.
  https://stackoverflow.com/questions/39877521/using-docker-compose-within-google-app-engine


  and creates image everytime.
  I'm looking for a way to use gcloud app deploy...  to directly grab a Docker address from Docker Hub and deploy straight to Google App Engine. If there's no way, that's fine but I thought I'd check here.
  I looked around and didn't see a way to do this directly. It seems that I have to push the container from Docker Hub to Google Container Registry then from there I can run gcloud app deploy --image-url=[HOSTNAME]/[PROJECT-ID]/[IMAGE] to deploy.
  ans) It might be a better idea to just use a Compute Engine instance. I can push a container from a public source or public/private from Container Registry.
      https://stackoverflow.com/questions/71446771/is-there-a-way-to-push-a-docker-container-from-dockerhub-to-google-app-engine
      >gcloud compute instances create-with-container VM_NAME \ --container-image DOCKER_IMAGE


IMP- uses https by default gcp giving us public ip, and then bind to the container http port running. 
     this is the power of serverless app instance gcp or azure app or aws app, gcp/cloud does all this by itself





