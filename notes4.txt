
----------------------------------------------------------------------------------------------------------------------------------------

0)normally, git pull in heroku, heroku gives an ip and expose its one port then run the app run command.
and maybe if want run a scheduled job to pull from github and run the app again 

1)but my ci/cd and cloud,
travis ci, whenever pushed to github, automatically builds the image and hosts.(doesnt wait for scheduled job)
and the gcp pull the image and restarts only those specific docker containers using those images

(
  2)if scheduled job with gcp instead of ci/cd and containers then just like heroku, but gcp gives better features like vnet, firewall, image/blob storage etc.
  3)also containerization needed as every service not dependent on another and each service can be updated/scaled independently
            (else different services run like different apps/processes on host-port if not using containers) 
  as ci/cd listens to that containers entry folder in github
  4)if scheduled job with docker in gcp instead of ci/cd then gcp will have to pull and build image everytime as running on containers not natively
   (means gcp is running the .sh commands job instead of ci/cd online shells)
)




----------------------------------------------------------------------------------------------------------------------------------------


Done with Docker, CI/CD(just shell scripts online shell) and Github(Dockerhub also same) at this point. only Cloud and TF left.





If you're using Azure CLI over a proxy server that uses self-signed certificates,
 the Python requests library used by the Azure CLI might cause the following error: SSLError("bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)",). To address this error, 
 set the environment variable REQUESTS_CA_BUNDLE to the path of CA bundle certificate file in PEM format.


export REQUESTS_CA_BUNDLE=/Users/abhinav.bhattacharje/mern/infrastructure-as-code-talk-docker-terraform/infrastructure-as-code-talk/sinatra-backend/proxy-cert-2.pem 


azure cloud - 
Azure Cloud Shell requires a storage account with Azure file share to persist files(like volumes on docker, else all file data and state deleted). 
Select an option below to mount a storage account
subscription - 54001000 abhinavbhattacharjee
snow subscription login using sso
resource group - testvm

Subscription: 54001000 abhinavbhattacharjee
Resource group: 88a798b0-9501-4368-ba51-65960c63f5ea/resourcegroups/cloud-shell-storage-centralindia
Storage account: csg10032002b8b2b06f
File share: cs-abhinav-bhattacharje-servicenow-com-10032002b8b2b06f
Region: Central India

>az account show
>az group list
>az group show --resource-group testvm
>az vm image list //all the default vm images(like packer images) for unix/windows etc on azure hub(like docker base images)
>az vm list
>az vm show -n testvm1 --resource-group testvm
>az vm create -n MyVm -g MyResourceGroup --image Ubuntu2204

just create your own azure , aws, and gcp account and use in personal laptop



-------------------------------------------------------------------------------

AWS ECS is just a logical grouping (cluster) of EC2 instances, and all the EC2 instances part of an ECS act as Docker host i.e. 
ECS can send command to launch a container on them (EC2). 


Docker is a utility you can install on our machine, which makes it a Docker host, and on this host you can run a vms which creates containers (same as virtual machines - 
but much more light-weight). To sum up, ECS is just about clustering of EC2 instances, and uses Docker to instantiate many containers/instances/virtual
 machines on these (EC2) hosts.


EC2 allows you to launch individual instances which you can use for pretty much whatever you like. ECS is a container service,
 which means it will launch instances that will be ready to launch container applications
 Elastic Container Service (ECS) is a Docker container orchestration service.

You can ask it to run one or more Docker images, either as an auto-scaling capable "Service" or as an ad-hoc "Task".
The services and tasks run on a "Cluster". Originally, a Cluster was a group of one or more pre-configured EC2 servers running ECS Cluster Agent. The Cluster Agent would schedule the containers on the EC2 server. These EC2 servers show up in your EC2 Instances list and are charged at regular EC2 per-minute costs - You can even SSH onto them like any normal EC2 server. If you wanted more capacity to run more Services or Tasks, or if you wanted resilience against EC2 failure, then you would more EC2 servers.
Around November 2017, AWS added ECS Fargate. Now a Cluster can run "serverless" without provisioning EC2 nodes(means provisioning EC2 handled by amazon instead of us based on ram/cpu).
 You simply define the amount of CPU and memory your Task or Service requires to operate, meaning that you just pay for the CPU and memory time rather than the EC2.

ECS is a container orchestrator just like Kubernetes or Docker swarm, EC2 is an Amazon Elastic Compute Cloud platform for Creating Virtual Machines.
 ECS allows you to run containers on either serverless environments (Fargate)(amazon handles the ec2s, we manage the containers) where you don't have to run any VM or in a non managed environments 
 where you host the containers on EC2 instances.(we manage both ec2s and containers on them)

https://stackoverflow.com/questions/40575584/what-is-the-difference-between-amazon-ecs-and-amazon-ec2




--------


https://www.slideshare.net/brikis98/infrastructure-as-code-running-microservices-on-aws-using-docker-terraform-and-ecs?ref=http://www.ybrikman.com/writing/2016/03/31/infrastructure-as-code-microservices-aws-docker-terraform-ecs/#152
https://github.com/abhinav455/abhinav455-infrastructure-as-code-talk-docker-terraform-aws.git

actual tf.main :-


module "rails_frontend" {
  source = "./ecs-service"

  name = "rails-frontend"
  ecs_cluster_id = "${module.ecs_cluster.ecs_cluster_id}"

  image = "${var.rails_frontend_image}"
  image_version = "${var.rails_frontend_version}"
  cpu = 1024
  memory = 768
  desired_count = 2

  container_port = "${var.rails_frontend_port}"
  host_port = "${var.rails_frontend_port}"
  elb_name = "${module.rails_frontend_elb.elb_name}"

  # Provide the URL of the sinatra-backend ELB using the same environment variable name and format as docker-compose
  num_env_vars = 2
  env_vars = "${map(
    "RACK_ENV", "production",
    "SINATRA_BACKEND_PORT", "tcp://${module.sinatra_backend_elb.elb_dns_name}"
  )}"
}
-------


inside tf module .main :-

# CREATE AN ECS SERVICE TO RUN A LONG-RUNNING ECS TASK
# We also associate the ECS Service with an ELB, which can distribute traffic across the ECS Tasks.
# --------------------------------------------------

resource "aws_ecs_service" "service" {
  name = "${var.name}"
  cluster = "${var.ecs_cluster_id}"
  task_definition = "${aws_ecs_task_definition.task.arn}"
  desired_count = "${var.desired_count}"
  iam_role = "${aws_iam_role.ecs_service_role.arn}"

  deployment_minimum_healthy_percent = "${var.deployment_minimum_healthy_percent}"
  deployment_maximum_percent = "${var.deployment_maximum_percent}"

  load_balancer {
    elb_name = "${var.elb_name}"
    container_name = "${var.name}"
    container_port = "${var.container_port}"
  }

  depends_on = ["aws_iam_role_policy.ecs_service_policy"]
}

# -----------------------------------------------------
# CREATE AN ECS TASK TO RUN A DOCKER CONTAINER
# -------------------------------------------------------

resource "aws_ecs_task_definition" "task" {
  family = "${var.name}"
  container_definitions = <<EOF
[
  {
    "name": "${var.name}",
    "image": "${var.image}:${var.image_version}",
    "cpu": ${var.cpu},
    "memory": ${var.memory},
    "essential": true,
    "portMappings": [
      {
        "containerPort": ${var.container_port},
        "hostPort": ${var.host_port},
        "protocol": "tcp"
      }
    ],
    "environment": [${join(",", data.template_file.env_vars.*.rendered)}]
  }
]
EOF
}
--------


In terraform providers
---deploy
1) Ecs_cluster manages each EC2 Instance that will run in the ECS Cluster

    aws_autoscaling_group -   size = 6
2) Ecs_instance/aws_launch_configuration is like a aws host vm( based on customised packer like image launch configuration we mention in ecs_instance)
         ecs_instance - image_id = "${data.aws_ami.ecs.id}" 
         data "aws_ami" "ecs" {  values = ["amzn-ami-*-amazon-ecs-optimized"],}   
         ecs_cluster manages this like t2.micro
3) inside Ecs_instance(host vm), we create Ecs_service(docker vm)  (they have elb load balacer configured to talk to other docker/host vm)
         //doesnt need any image as the standards docker just need config
          all ecs_service managed by cluster ecs_cluster
4) inside Ecs_service(docker vm) we create aws_ecs_task_definition(docker container/task)  (based on customised docker image we mention in aws_ecs_task_definition)

  2.2) Ec2 is like a aws host vm thus needs ec2 packer image(like aws_launch_configuration like t2 micro)


-----redeploy
1)here whenever the docker image needs to be build again and put on dockerhub, 
2)there are already running ecs cluster, ecs instances/host vm, running ecs services/docker vm
  and inside that running containers. we just need to remove and again run/ compose-up the containers on aws using
  ecs_cluster commands/ new terraform file mayb/ansible(configure)/ just for containers not provision  whole vms, clusters, load balancers etc.
3)but just for easy way they told us to again terraform init,plan, apply to reprovision all infra and 
   resources, will obviously then use need docker built images from the hub

   3.2) all these steps can also automate with shell jenkins ci/cd scipts .yaml .sh
export AWS_ACCESS_KEY_ID=...
export AWS_SECRET_ACCESS_KEY=...
vars.tf - main vars.tf tweak if want docker images and their ports just, 
          the security, instance count/sizes etc in other modules
>terraform init,plan,apply 
init initializes the provider plugins and the modules, how our new infra looks like
plan compares with old infra using old .tfstate file(stored in secure s3/blob), makes a plan how to create/update old infra
    to see any changes that are required for your infrastructure
apply makes the changes according to the plan


>terraform plan -out tfplan
      data.aws_subnet.default[2]: Read complete after 0s [id=subnet-0845d5267b4ec1b49]
      data.aws_subnet.default[1]: Read complete after 0s [id=subnet-0d85a1d6e98337810]
      module.rails_frontend.data.template_file.env_vars[0]: Reading...
      module.rails_frontend.data.template_file.env_vars[0]: Read complete after 0s [id=187c415a5554e3f8f89a9a778ddcd5445a4df052ae28e1b501a6250a79b1220d]
     
      Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
        + create
      <= read (data resources)
      Terraform will perform the following actions:
        # module.ecs_cluster.aws_autoscaling_group.ecs_cluster_instances will be created

        # module.sinatra_backend_elb.aws_security_group_rule.all_outbound_all will be created
        + resource "aws_security_group_rule" "all_outbound_all" {
            + cidr_blocks              = [
                + "0.0.0.0/0",
              ]
            + from_port                = 0
            + id                       = (known after apply)
            + protocol                 = "-1"
            + security_group_id        = (known after apply)
            + security_group_rule_id   = (known after apply)
            + self                     = false
            + source_security_group_id = (known after apply)
            + to_port                  = 0
            + type                     = "egress"
          }

      Plan: 25 to add, 0 to change, 0 to destroy.

      Changes to Outputs:
        + rails_frontend_url  = (known after apply)
        + sinatra_backend_url = (known after apply)
      ╷
>terraform apply "./tfplan"


Done with Docker, CI/CD(just shell scripts online shell) and Github(Dockerhub also same) at this point. 
Cloud and TF also done(just shell scripts to configure and provision infra, instances, networks for deploy etc)
    
----------------------------------------------------------------------------------------------------------------------------------------


hypervisor vm for each application before vm.
application in a  vm will be restricted/dependent on the os and libraries of the os(os+libraries+app)

thus instead of many vm/os for each app, we have one single docker server virtual vm(thus same OS across all apps) running each app in a isolated container(libraries+app) with all
libraries installed

yaml,xml, json data serializer language
yaml files for containers and github actions ci/cd

-------
Building for production  //while dev build and run at the same time
>npm run build //>npx react-scripts build

    File sizes after gzip:

      278.43 kB  build\static\js\main.6681b3c8.js
      28.45 kB   build\static\css\main.feaf2411.css

    The project was built assuming it is hosted at /.
    You can control this with the homepage field in your package.json.

    The build folder is ready to be deployed.
    You may serve it with a static server:

      npm install -g serve
      serve -s build

    Find out more about deployment here:

      https://cra.link/deployment

in production, just add the built bundle.js and bundle.css code build using webpack etc as just serve using a static server
no need to do npm run start as does both 1)build again and again before running in prod and 2)servers using react cra server

after build we dont need cra server(although can use), can use vite/nginx for static serve

building better in prod as optimised build and can use any server and dont need to build and run at same time but one after another


CMD ["npm", "run", "server"] #not using nginx but our own cra static server to serve 

for nginx, in same dockerfile add -  
# production environment
FROM nginx:stable-alpine
COPY --from=build /app/build /usr/share/nginx/html
COPY nginx/nginx.conf /etc/nginx/conf.d/default.conf
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]

Here, we take advantage of the multistage build pattern to create a temporary image used for building the artifact – the production-ready React static files 
– that is then copied over to the production image. The temporary build image is discarded along with the original files and folders associated with the image. 
This produces a lean, production-ready image.


>docker build -f Dockerfile.prod -t sample:prod .
>docker run -it --rm -p 1337:80 sample:prod

---
in docker-compose, not necessary to use an already build image, can build from dokcerfile and run at same time(anyways iterative build from our built image in docker-compose only)
(instead of building from our built image, building from alpine base image)

version: '3.7'

services:

  sample-prod:
    container_name: sample-prod
    build:
      context: .
      dockerfile: Dockerfile.prod      //building again from dockerfile instead of using prebuilt image
    ports:
      - '1337:80'

>docker-compose -f docker-compose.prod.yml up -d --build      

>docker run imagename ls //runs containers and lists all filesystem in it like we are in the terminal of container -it -tty like, but background only -d
>docker-compose build //buid image from dockerfile mentioned in docker-compose file 
>docker-compose down //delete the built image

>docker-compose up   //automatially detects the built image
>docker-compose run server sh  //goes into container shell

node_modules in windows wont work in dockercontainer linux need to reinstall
----------

dotenv module loads the variables from .env file into process.env module
require("dotenv").config();  //console.log(process.env.MY_NAME)

------
depends_on:    #dont want to run container client before server 
  - server








