-----------

used vite create frontend server(OR create-react-app   react-scripts start) to bundle js
vite does not have anything to do with testing


so useeffect is react implemented like this 
export function setupCounter(element) {
  let counter = 0
  const setCounter = (count) => {
    counter = count
    element.innerHTML = `count is ${counter}`
  }
  element.addEventListener('click', () => setCounter(counter + 1))
  setCounter(0)
}

all fns are inside a main fn, the inner fns are created everytime in each render but the 
state usestate variables are in main fn, thus they stay the same as that fn only created once


--------------------------------------




Vite is a local development server written by Evan You, the creator of Vue.js, 
and used by default by Vue and for React project templates. It has support for TypeScript and JSX. It uses Rollup and esbuild internally for bundling

When cold-starting the dev server, a bundler-based build setup has to eagerly crawl and build your entire application before it can be served.
Vite improves the dev server start time by first dividing the modules in an application into two categories: dependencies and source code.
Native EMS is on-demand by nature. 
Based on the browser request, Vite transforms and serves source code on demand. If the module is not needed for some screen, it is not processed.

npm is used to manage dependencies and packages in your project, while Vite is a build tool that optimizes the development experience. 
However, they serve different purposes and have different features. Vite uses esbuild and is used to bundle the code and npm is just a package manager like yard.
Prefer Vite Over Create-React-App (CRA)

vite/create-react-app bundles our code, sets up frontend server to listen on 0.0.0.0, and hosts our code(returns our bundled js code with html and css) 
to the browser on get request to our endpoint.
CRA uses a webpack under the hood. The webpack bundles the entire application code before it can be served. vite bundles dynamically. and only the non source code is there

but i run my frontend server using >npx react-scripts start

on frontend endpoint only the frontend index.html code is returned to postman. where does the js and the css go? how does DOM create components from them?

In summary, when you access localhost:3000, the server returns the index.html file. 

The index.html file includes <script> tags that point to your bundled JavaScript files (e.g., bundle.js) and <link> tags for CSS files.


 <script defer src="/static/js/bundle.js"></script>  - in our index.html, this script tag is added by create-react-app/vite which includes the complete js bundle
 https://stackoverflow.com/questions/44050851/create-react-app-change-src-of-bundle-js-in-development
  get localhost:3000//static/js/bundle.js




Loading JavaScript and CSS: - 
The browser automatically requests these files from the server.
The browser then loads the referenced JavaScript and CSS files, 

Executing JavaScript: - 
Once the JavaScript files are loaded, they execute in the browser. This JavaScript is generated by the React build process (using tools like Webpack or Vite) and includes all the React components and application logic.
The JavaScript code typically includes React, ReactDOM, and your application code.

Rendering the React App:- 
ReactDOM renders your React components into the DOM element specified in the index.html (usually a <div id="root"></div>).
using native js dom manipulation fns like 
  document.querySelector('#app').innerHTML = ``
  element.addEventListener('click', () => setCounter(counter + 1))
React's rendering process involves creating and updating the DOM dynamically based on the state and props of your components.

As the React app runs, it may make additional network requests (e.g., to fetch data from APIs), respond to user interactions, and update the DOM accordingly.
React manages this process efficiently using its virtual DOM, ensuring that only the parts of the DOM that need to change are updated.


This process allows the complete JavaScript code bundles and CSS to be loaded and executed, 
resulting in the fully rendered React application you see in the browser.

-----------




-----------------------------------------------------------------------------------------
browser cant run ts, ts = ts(strongly typed, interfaces, generics)+es6+js, js+es6 i use in react, js(prototypes,dynamic types, constructor fns) i use in node  

ts - OOP, strongly typed- static and dynamic typing both
rovides classes, visibility scopes, namespaces, inheritance, unions, interfaces
TypeScript utilizes concepts like interfaces and types to define the data being employed
Static Typing
Static Typing means wherein the developer has to declare the variable type.
Supports ES6 (ECMAScript), which provides a simpler syntax for handling objects and inheritance.
js - scripting, loosely typed only dynamic typing

ts need compilation, js doesnt need compilation
Whether JavaScript is compiled or interpreted depends on the environment in which it is run.
 If it runs in older browsers, it's interpreted. If it runs in modern browsers, it's compiled just in time
compiled programs are not human readable and are specific to the architecture of the targeted runtime
Not only is it hard to get all ECMAScript engine vendors to agree on a common standardized bytecode format, but consider this: it doesn't make sense to add a bytecode format 
for only ECMAScript to the browser. If you do a common bytecode format, it would be nice if it supported ActionScript, VBScript, Python, Ruby, Perl, Lua, PHP, etc. as wel


React uses js. But the react framework and 3rd party libs itself is written in typescript. And the reactscripts start first transpiles ts to js code using tools like babel. 
And the transpiled js is hosted to browser. And browser does JIT of normal js running.
Same nodes does JIT of normal js
React Native uses the same core functionality as react, but once it has figured out the changes that need to be made, rather than update the dom (ie, the webpage), 
it updates native components for android or ios. React native thus lets you write native phone apps, using the syntax and tools that are familiar to react developers.
















-------------------------------------------------------------------------------------------------------------------------------------
js
var Person = (function()
{
function Person(personName)
{
this.name = personName;
}
Person.prototype.name= function()  //can mayb act like tostring dont know
{
return "My name is " + this.name;
}
return Person;
})();

IIFE (Immediately Invoked Function Expression)
The IIFE pattern is used to create a local scope and immediately execute the function. or use anonymous fns in es6
This helps in avoiding polluting the global namespace.

var Person = (function() {
    function Person(personName) {
        this.name = personName;
    }
    Person.prototype.getName = function() {   //or we can define the property in the main object itself like - Person.getName = fn()
        return "My name is " + this.name;
    }
    return Person;
})();

// Usage
var person = new Person("John");
console.log(person.getName()); // Output: My name is John

ts
class Person
{
private name: string;
constructor (private name: string)
{
this.name = name;
}
name()
{
return “name is “ + this.name;
}
}

in js we dont have classes by default.
Yes, JavaScript does have classes. They were introduced in ES6 (ECMAScript 2015) to provide a cleaner syntax for object-oriented programming.
but those classes are mainly an abstraction over the existing prototypical inheritance mechanism — all patterns are convertible to prototype-based inheritance. 

-------------------------
prototype - 
Every object in JavaScript has a built-in property, which is called its prototype. The prototype is itself an object, so the prototype will have its own prototype, making what's called a prototype chain.
 The chain ends when we reach a prototype that has null for its own prototype.

 When you try to access a property of an object: if the property can't be found in the object itself, 
 the prototype is searched for the property. If the property still can't be found, then the prototype's prototype is searched,
 ' and so on until either the property is found, or the end of the chain is reached, in which case undefined is returned.

looks for toString in myObject
can't find it there, so looks in the prototype object of myObject for toString
finds it there, and calls it.

Object.getPrototypeOf(myObject); // Object { }

define a property in an object, when a property with the same name is defined in the object's prototype
myDate.getTime = function () {
  console.log("something else!");
};

--
const personPrototype = {
  greet() {
    console.log("hello!");
  },
};

const carl = Object.create(personPrototype); //Object.create(prototype)
carl.greet(); // hello!

https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/Object_prototypes

IMP - thus prototypes can used for inheritance. prototype is the base class and the main obj is the child class.
        and this is the way ts inheritance and class are transpiled to js prototypes.



---------------------
thus everything in js is an object or a function or a elementary type.
 (
  1)fn can become object also if we define any properties in it,    
   eg f=fn(); f.nme="abc";  means f={default: fn(), nme:""abc}
   when we just call f(), the default fn is called. 
   we call f.nme, nme key in object f is called 
    var f = function() {return "def";};
    f.nme= "abc";
    console.log(f());   //def
    console.log(f.nme); //abc
    console.log(f.prototype); //{}
    console.log(f.hasOwnProperty("nme")); //true
    Object.keys(f); // ['nme'] array -  0: "nme"length: 1[[Prototype]]: Array(0)
    f.prototype.def="kry";
    console.log(f.prototype)  // {def: 'kry'}
    console.log(f.prototype.def); // kry
    console.log(f.def); //undefined        /////here is the difference b/w fn nd   //as this is a fn definition not object/fn object doesnt by default search in .prototype
    var fobj = new f();
    console.log(fobj()) //error fobj is not a fn but "def" string    //as this is fn return value/fnobj dont know
    console.log(fobj.nme); //undefined //since string type can have property as not object type
    console.log(fobj.def); //kry      //as this is normal type thus searches in prototype
    fobj.hasOwnProperty("nme"); //false
    fobj.prototype.hasOwnProperty("def"); //TypeError: Cannot read properties of undefined (reading 'hasOwnProperty') at <anonymous>:1:16
    fobj.hasOwnProperty("def"); //false
    console.log(fobj.__proto__); //{def: 'kry'}   //the proto object
    console.log(fobj.__proto__.hasOwnProperty("def")); //true
    console.log(fobj.prototype); //undefined

   .__proto__ is the actual object that is used in the lookup chain to resolve methods, etc. 
   .prototype is the object that is used to build __proto__ when you create an object with new

    ( new Foo ).__proto__ === Foo.prototype 
    ( new Foo ).prototype === undefined
    So prototype is not available on the instances themselves(new FOO) (or other objects), but only on the constructor functions(FOO).
    prototype is only available on functions since they are derived from Function, Function, and Object but in anything else it is not. 
    However, __proto__ is available everywhere. 

        very confusing just do straightforward 
    if need to use fn objects, define in prototype(normal properties wont work) and use them implicit or difrectly via __proto__
    if need to use fns directly, define in property(prototypes/__proto__ implicit wont work as not object/elementary type) 


    https://stackoverflow.com/questions/9959727/what-is-the-difference-between-prototype-and-proto

   2)but if we do f="abc". then elementary type then cant define other properties like f.val="def" 
    var g="abc";
    g.df="def";
    console.log(g);    //abc
    console.log(g.df); //undefined
    console.log(g.prototype); //undefined
    console.log(g.hasOwnProperty("df")); //false

 )


we dont have class in nomal js. thus there is nothing known as object of a class.
we only have fns and object types. thus only one memory present and all vars point to that fn/object memory
but this is same for all languages. if we want new memory, use new keyword(create the new return value with prototype/fn obj dont know)
works with fns (and objs too mayb).

As we know, functions in JavaScript are objects and they have their own properties and methods (more properly, function instances, inherited from Function.prototype).
First of all, it's important to realise that standard function properties (arguments, name, caller & length) cannot be overwritten. So, forget about adding a property with that name.
Adding your own custom properties to a function can be done in different ways that should work in every browser.

















-----------------------------------------
mongodb, stripe, github





----------------------------------------------------------------------------------------------------




------------------------------------
RSA
https://www.youtube.com/watch?v=Pq8gNbvfaoM 
server data sent is encrypted using private key and decrypted using public key at client
and client sends back encrypted data using public key(important info of client) and server decrypts the data using its private key

The pair {N=p*q,e} is the public key, while {d} is the private key. find d by d*emod(p-1*q-1)=1. 
thus we know n,e (cypher = message^emodn) . if we somehow extract p,q from n, we can find d from formula d*emod(p-1*q-1)=1. easily. 
finding mod in O(p-1*q-1) as just iterate all numbers 1 to (p-1*q-1) to get d satisfying d*emod(p-1*q-1)=1.
and get message=cypher^dmodn.  

https://www.geeksforgeeks.org/multiplicative-inverse-under-modulo-m/

VERY IMP - cypher encrypt(^emodn) and decrypt(^dmodn) works as we are finding  d from (e and totient(p-1*q-1)). thus totient is necessary to the same value we get after 
encryption and decryption. also We know d exists due the constraints put on e that e<t and e and t coprime.

IMP - also make sure d and totient(p-1*q-1) are coprime. 
if coprime, if p and q know can find mod inv e d*emod(p-1*q-1)=1  instead of O(p-1*q-1)iterating all numbers O(2^1024) impossible(O(10^9) takes 1 sec) 
use  extended eucledian algorithm O(log(p-1*q-1))
thus n is 1024 bits, p-1*q-1 also near 1024 bits.  means time taken = logbase10(2^1024) to find mod<1024, thus can be done in 1024 operation O(10^3) <1sec today. 
thus finding mod is easy, factorising n and getting p and q is hard.

//(if p-1*q-1 prime, can use fermats little theorem, d=e^(x-2) , x=p-1*q-1). O(log(p-1*q-1), not the case here as p-1*q-1 can be non-prime)


MAKERS OF RSA
https://www.quora.com/What-are-the-potential-consequences-of-finding-out-all-the-prime-factors-of-an-RSA-number
1)makers of RSA took any prime p,q such that p*q>=2^1024. real life easy only
2) got t=p-1*q-1. easy.
3) got e by iterating 1 to p-1^q-1(O(2^1024)*log(p-1*q-1)), such that e and p-1*q-1 are coprime(check if a and b by gcd(a,b)=1. log(p-1*q-1)= logbase102^1024). 
        (will never be able to finish iteration) but we get answer soon enough. real life easy only
4) got d by eucleadean algorithm d*emod(p-1*q-1)=1 O(logbase10(2^1024)). easy .

keep e with server private key, publish n,d as public key.    cyphertext=message^dmodn.
5)  In practice, p,q, and t(t less size thus can find p-1 and q-1 factors in sufficient time) are completely erased as soon as steps 1–4 are complete.

Generate RSA private and public key using openssl

------------------------------------------------------------








TSL/SSL certificate




https://stackoverflow.com/questions/38571099/how-can-i-set-the-certificates-in-curl
>curl --cacert <path of ca.pem> ...
>curl --with-ca-path=DIRECTORY
>export SSL_CERT_FILE=/path/to/ca.pem //so that dont need to pass ==cacert parameverytime with curl
>export SSL_CERT_DIR=/path/to

dd your rootCA.pem in /usr/share/ca-certificates directory.(default directory)
After that update your certificates with: >update-ca-certificates --fresh
>curl --cacert path/CERTIFICATE.cert GET "URL".
tell the program to send a client side cert to the server, which is processed at the TLS layer, and is invisble to the HTTP layer.

can use self signed certificate for testing, instead of certification authority
> openssl genrsa -out myprivate.pem 1024
> openssl rsa -in myprivate.pem -pubout > mypublic.pem

----
You can add this to your .bashrc/.zshrc or .bash_profile/.zprofile file to make this permanent.
~/.zshenv
Scripts source only zshenv.
Interactive non-login shells source zshenv and zshrc.
Interactive login shells source zshenv, zprofile, zshrc, and zlogin. 
.zprofile is sourced upon login.
.zshrc is sourced upon starting of a new shell.
.bashrc or .zshrc are typically used to configure the look and feel of the interactive shell. For example, customizing the prompt, font color, etc. could be done here. 
----


in https website, in chrome at start of url click to see the certificate
or go to security tab in chrome
 

certificate is very tied to the website's domain name. 
CA company checks the credibility of the certificate applicant and whether the domain name has been purchased by the applicant.

something apple keychain in mac what is it?

.pem file, when open in notepad only shows encrypted certificate token, 
Privacy Enhanced Mail (PEM) is a Base64 encoded Distinguished Encoding Rules(DER)
PEM file is human readable as it uses 64 printable characters for encoding. actually when decode, we have binary data
It is easy to share PEM file.

but when open in chrome shows all the  data like - 
1)issuer - CN = Zscaler Intermediate Root CA (zscalerthree.net) (t) ; OU = Zscaler Inc.; O = Zscaler Inc.; ST = California; C = US, CN = cloud.mongodb.com

2)RSA PUBLIC KEY (Public key contains modulus and public exponent.)
  Modulus (2048 bits):                                      = N(modulus)
    C7 23 AA 2A D3 AF 59 B1 46 97 80 69 1F 76 51 C3
  B0 E7 A2 A3 85 9D 9E 2E 6F 66 55 AF F7 68 E0 7D
  A6 B0 B3 C0 B5 2C 78 69 9A 2F 26 1E 5F 4B AA 80
  5B DB 9D 17 33 A9 94 10 54 26 AA 01 B5 FF 71 6F
  B4 B4 BB 6D 43 AE BB 03 6D B5 07 96 A3 32 4C FF
  39 E1 D1 58 0C A7 C8 E7 BD B9 62 F6 44 32 A4 4F
  22 6D 21 AE D7 82 A2 71 8A 5C 29 8F AB F5 8A 16
  D9 40 3F 33 E9 32 C2 89 1D F5 6B 43 BD 2F 55 5E
  3B 74 66 18 39 04 2C 64 76 08 97 2D BF 40 E2 E4
  2A 62 FD BF AF 9E D6 1F DE 58 A6 A2 10 D8 5F C4
  D2 44 DD A2 22 8E EB CD 1D C7 77 7E 20 EF 03 6F
  CC A3 CB AF 4F F8 8B 7D 38 15 30 78 FE C5 B0 93
  31 CB 85 32 AB 32 66 E3 E4 E7 05 86 85 BC 28 29
  DB 6B 4F 98 5A 7C 4D D0 96 6D B9 33 7A 61 94 D6
  64 22 E6 B2 15 33 23 C2 38 C9 84 68 6C 7B 7D 1C
  38 5D 5F 44 22 2D AE 21 56 65 30 97 63 E3 DD 9F

  Public Exponent (17 bits):                                  = D(public key)
  01 00 01
  
  
  bin = 10000000000000001, dec = 65537 (D=public key) hexadecimal means 4 bits together binary, 0-15 decimal
3) Not Critical
    DNS Name: cloud.mongodb.com
    DNS Name: opsmanager.mongodb.com

4) PKCS #1 SHA-256 With RSA Encryption 
      SHA256 fingerprints - Certificate	40b31d28b57818167b800721af2290df851bed2ca6fde7386f25d692c63ae9ca
                            Public key	0168313c0f37119d4e8d2cf103c6e6123dfe8fa013d6230d1c78e8f4e6bbbea4

    https://medium.com/@bn121rajesh/understanding-rsa-public-key-70d900b1033c#:~:text=Public%20key%20contains%20modulus%20and,to%20decode%20the%20original%20value.
   
   IMP - SHA256 HASH of the request also sent with public sha key so that receiver can regenerate hash from gotten request using public key 
      and thus if hash matches then knows that the request is not tampered/lost data.


5) validity period Issued On	Sunday 7 July 2024 at 09:11:00 ; Expires On	Sunday 21 July 2024 at 09:11:00

--------


Generate RSA private and public key using openssl
> openssl genrsa -out myprivate.pem 1024
> openssl rsa -in myprivate.pem -pubout > mypublic.pem
>openssl rsa -pubin -inform PEM -in mypublic.pem -outform DER -out mypublic.der
> xxd -g 1 -u mypublic.der | cut -c -57
00000000: 30 81 9F 30 0D 06 09 2A 86 48 86 F7 0D 01 01 01
00000010: 05 00 03 81 8D 00 30 81 89 02 81 81 00 D1 14 D5
00000020: 3E FB DD DA 12 FC F7 71 5F 0B 49 43 FD 89 BD E2
00000030: 53 14 FB 4A E7 DD 55 77 20 54 52 BD 33 70 58 CE
00000040: FA E8 03 5B 8E FE 96 AB 14 E2 40 05 39 0D 85 33
00000050: CF 3F FE A6 8E B5 36 08 F1 19 27 5C C8 92 96 92
00000060: 34 9A EB 86 9A 7F AC D3 0E F6 7C 8B 60 F1 AC F4
00000070: C7 DD 06 25 94 3E 61 D6 E6 66 35 A0 3D 32 7B 89
00000080: B2 D2 D1 2C 1C E9 60 1C 2F 00 84 0F 0E B6 21 EB
00000090: E8 86 34 6A 05 E6 1F FC B9 62 64 96 6F 02 03 01
000000a0: 00 01
>openssl rsa -pubin -inform PEM -text -noout < mypublic.pem
Public-Key: (1024 bit)
Modulus:
    00:d0:c6:a7:97:e0:ef:07:b3:2b:1d:7d:f5:52:81:
    3d:e7:16:ee:49:73:bb:88:f2:de:4b:0d:ad:38:e8:
    5b:a5:e5:12:02:6b:68:4c:9a:87:f6:6f:76:89:b7:
    6e:17:2f:52:05:68:00:06:38:81:66:2a:20:d6:f8:
    bc:a7:d2:f0:bc:87:6a:8a:5a:69:cf:e4:0f:7c:d5:
    e4:20:c5:66:18:af:37:e4:33:d5:7a:62:cb:f8:6b:
    e3:fe:be:26:3c:ed:23:46:4e:02:48:2d:3a:44:9f:
    80:51:8e:76:76:18:4f:89:79:7a:cd:3d:de:95:28:
    15:45:88:6c:92:46:32:1f:c1
Exponent: 65537 (0x10001)
----------

theory - 


Transport Layer Security (TLS) is a network protocol that establishes an encrypted connection between a client and server over an untrusted network. 
HTTPS is an implementation of TLS encryption on top of the HTTP protocol, which is used by all websites.
HTTPS uses TLS to encrypt and digitally sign HTTP requests and responses, making it more secure than HTTP

HTTPS = HTTP+ TLS/SSL(encryption cert and session keys, still only TLS/SSL susciptible to man-in-middle attacks) + CA verify(identity)

TLS -  A)server have a certificate having (n,e) passsed to client. 
       B)client verifies the certificate using CA to check if cert valid. and if yes, gets [n,e] then sends seed value encrypted in [n,e] 
             to check if actually server is there identity as it can decode seed using [e]

        // .passed using key exchange algo(not exact algo used here), (used for encryption and identity(digitally signed done due to identity))
                                                                      //  able to decode means yes the identity is correct
       // both share their public key with each other  
       // cypher = msg^PrivateKeyA^publickeyB
       // msg = cypher^privatekeyB^publickeyA = msg^PrivateKeyA^publickeyB^privatekeyB^publickeyA = msg 

        C) client send the secret, and keys created using the seed secret and both have encryption and decryption keys generated from secret.


        now all HTTP encrypted using the above mechanism. (no actual symmetric key shared, but seed shared from client to server from which encryption and decryption keys
                                                          created in both places)

        D) the public and private key used for key exchange algo comes from certificate. 
           after this, IMP -  the certificate keys not used(as client doesnt have certificate private key) but the
            newly generated session keys(encryption and decryption keys or symmetric key sent from client whatever dont matter) used. 
           //Thus server and client(in chrome browser google gives the certificate implicitely) both need private certificate(e) and public certificate(n,d). 
            
            and sha256 encryption key to check has if data has not been tampered.

     Thus is this way, https works.

https://howhttps.works/certificate-authorities/
https://www.youtube.com/watch?v=ZkL10eoG1PY

       E) Self-signed certificates can be useful for testing, and intranets, but you should avoid using them on public sites.
          Self-signed certificates can be forged. Basically, they say 'Trust me, it's me, I promise!'. A trusted certificate says: 'Trust me, an authority verified me'.   
          IMP - means CA verification step not there in DEV ENV, rest everything there. 
              (handshake getting cert in client from server, sending seed from client in public cert key encrypt, 
                both parties generating session key(encryption and decryption keys), 
                and then finally encrypting http data using the session keys-creating hash using cert sha256 key to verify tampering and thus TLS/SSL(depreciated) on HTTP done) 

            But CA verification step is only the main feature of https(HTTPS+TLS+CA) so that man-in-middle attacks(identity) dont happen
               (as attacker can give his self signed/real certificate/public key and have private key to decode the client sent session key).
            Thus in DEV,  CA verify happens but the CA is server domain itself, all HTTP and TLS features and functionalities of HTTPS there. 
            But security tested is zero as per HTTPS as main feature CA  tested, but not with proper CA. 
            Chrome has proper list of CA domains that it trusts, the self sign server domain wont trust.


          CA
          https://www.youtube.com/watch?v=EnY6fSng3Ew
          first step is CA cert step to know that server and its public key[n,e] is correct. then TLS+HTTP. CA cert is just like a chain of trust with CA authority(Digicert) 
          and middle/intermediate CA authority(cloudflare). for hacker just like stock not worth it and very big hassle to hack like high commision fees and low win rate and payout ratio. 
          1)CA cert signs the public key of server with its private key(doesnt send the private key but just encrypts public key with it), 
            and then attaches it to cert and gives cert back to server. thus CA cert private key[d] is always safe, most important to protect.

            also side note CA company checks the credibility of the certificate applicant and whether the domain name has been purchased by the applicant. 
            also browser checks the domain name of the website is the same as in the certificate 
          2) server sends this cert to client. client gets CA public key and decrypts the encrypted public key of server in cert to get public key of cert.
            and verifies this with the actual public key server sends during TLS handshake and if correct then proceeds with 
            TLS handshake(sending symmetric key to both client/server etc.)   + HTTP+TLS communication.
         
           Note- 3) CA is the server itself in DEV ENV. in DEV ENV self signed CA, the CA domain and server domain field in certificate is the server domain itself.
             works with DEV ENV but in PROD, Chrome has proper list of CA domains that it trusts, the self sign server domain for CA domain chrome wont trust. 

             Root CA is self signed only and like DEV chrome trusts it by default during HTTPS of other server domains.(issuer and owner(server) domain same in CA's own cert).
             for intermediate, (issuer=rootCA, owner=intermediate). thus that too need to verify if intermediate correct.

             we get certificate hierarchy all 3 cert root CA cert(self signed issued by itself), Intermediate CA cert(issued by root ca), server cert(issued by intermediate)


            //////////////////
            in dockerfile
            >apk --no-cache add curl ca-certificates //apk package manager for linux alpine, install curl and ca-certificates package.
            ca-certificates: This package provides the CA root certificates, which are essential for SSL/TLS connections to work properly
            It contains the root certificates of trusted CAs, which are used by applications to verify the identity of remote servers and establish secure connections.
            These certificates are used system-wide by various applications and libraries, such as web browsers, curl, wget, and others, to ensure secure communication.

            as for getting public key of rootCA, we dont go to rootCA that time, we have the certificates of rootCA stored in system and installed from trusted source
            (in mac look at keychain access app) and get public key of that rootCA from the stored certificate of that rootCA
            /////////////////   
            
              https://stackoverflow.com/questions/73620124/update-ca-certificates-command-not-found-despite-ca-certificates-being-installed

-----------------------------------------------------------------------------------------------------------                





----------------------------------------------------------------------------------------------------------------------------------
github

>git config --global user.email "you@example.com"
>git config --global user.name "Your Name"

https://docs.github.com/en/migrations/importing-source-code/using-the-command-line-to-import-source-code/adding-locally-hosted-code-to-github
>git init -b main
//>git remote add origin REMOTE-URL  (connect a web repo to our local repo to track changes etc)
//>git branch --set-upstream-to=origin/main main
//>git pull
>git add .   # Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'.
>git commit -m "First commit"    # Commits the tracked changes and prepares them to be pushed to a remote repository. 
                                 #To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again.
Create a new repository on GitHub.com.  and copy its url (this is manual step)
>git remote add origin REMOTE-URL  (connect a web repo to our local repo to track changes etc)
>git push -u origin main
>git pull

Github authorizes vscode using oauth to access my data (grant and access token, auth and resource server). thus vscode able to see my repo.
----------------------------------------------------------------------------------------------------------------------       












https://himanshu-sheth.medium.com/jenkins-vs-travis-which-ci-cd-tool-is-best-for-you-754d67222ed1
https://docs.docker.com/guides/workshop/

Q)difference between builing in CI/CD and just runnings jobs to pull from github and rerun npx run scripts 
    (jenkins makes docker container(like another small vm) in VM do this only,  
        1)to pull from github and run again, then create image and then shut down docker and send final build image/file to host) 
        or 2) or the host vm itself(which is provisioned using terraform and configured using terraform/packer) 
                running build/application deployed on docker container(container is provisioned and configured using docker .dockerfile) 
                thus jenkins just makes docker pull the new app code from github and build/compile(into executable) and run the app again

ans) A)git pull and build/compile(into executable) into a docker image is what the CI/CD does. 
     but diffrence is 
     B)CI/CD builds the whole image and configurations of the container along with app into an executable.  
     C)[build/compile(into executable) and run the app again] only saves app executable state, not the container info to run it, thus if running on same 
        preconfigured server everytime good. 

     D)but better to have whole build docker image ready so that can just run image in any server/vm inside container. 
     E)and the server/vm in turn needs to be run/created from its own packer built/configured image. or terraform provision everytime.   

      CI means building the image everytime. 
      CD means running docker-compose(on the CI image) on local or complete terraform apply to provision vm and containers(from the CI image) inside them everytime.



----------------------------------------

https://github.com/brikis98/infrastructure-as-code-talk/tree/master

1)Install Docker.
2)docker-compose up (reads all images and deploys into running containers (already created from dockerfiles and packaged all the docker app)
3)Test sinatra-backend by going to http://localhost:4567.
4)Test the rails-frontend (and its connectivity to the sinatra-backend) by going to http://localhost:3000.

The docker-compose.yml file mounts rails-frontend and sinatra-backend folders as volumes in each Docker image, 
so any changes you make to the apps on your host OS  (meaning to the code? the image is same no? and no CI after commit then how will change autamatically?)
  will automatically be reflected in the running Docker container. 
This lets you do iterative "make-a-change-and-refresh" style development.

5) An example rails-frontend microservice that makes an HTTP call to the sinatra-backend and renders the result as HTML. 
   This app includes a Dockerfile to package it as a Docker container.

6) To deploy the microservices to your AWS account, see the terraform-configurations README.

7) BUILD - Follow Docker's documentation to create your own Docker images and fill in the new image id and tag in:
    docker-compose.yml: the image attribute for rails_frontend or sinatra_backend.
    terraform-configurations/terraform.tfvars: the rails_frontend_image and rails_frontend_version or 
                                              sinatra_backend_image and sinatra_backend_version variables.





   0)They already built the image from dockerfile in frontend and backend and put it on dockerhub.
   inside dockerfile - >ARG DIR
   >docker build --build-arg DIR=$(PWD) //An ARG instruction goes out of scope at the end of the build stage where it was defined. 

    Building docker image is just like building packer image(just instead of ap+vm config we have app+container config). 
    Thus in building image docker creates container, configures it, builds and runs app, configures it, and then saves its image and shuts down the container.
    so that the image can just be run/started easily next time. 
     (docker start [container name]; next step, after docker run; to store extra last run state not configured in image)
    
    after we build the image, we run the built image using


   A)>docker run -d -P an_image    //or use -p 80:80 and inside dockerfile EXPOSE 80
     >docker run -i -t --rm -p 80:80 nginx //docker run -i <image> <app>, last argument (<app>) will get your input.  app is nginx here, no image?

   ///////////////////////////////////
   https://stackoverflow.com/questions/48368411/what-is-docker-run-it-flag
    -it is short for --interactive + --tty. When you docker run in foreground with this command it takes you straight inside the container.
       allows you to interact with /bin/bash of the container using your own terminal. 
       docker run -i (interactive) this means that your terminal will transfer your input to container (app in container), till we press ctrl+d
       docker run -t (tty) your container's output is attached to your terminal. Seems it mostly about formatting output (for bin/bash, try >ls and see)

       When you type docker run -i <image> <app>, last argument (<app>) will get your input.
       echo "my input" | docker run -i <image> > output.txt    //output format notimportant
       docker run -t my-image | my-processing-script.sh        //no need of input but need output in specific format

    -d is short for --detach, which means you just run the container and then detach from it. Essentially, you run container in the background.

    Ability to get access into the shell inside the container and port forwarding are the two most commonly used Docker functionalities.
     Port forwarding allows you to access applications running inside the container through your host machine’s IP address (or localhost address).
    For example, to run a NGINX server inside a container and expose it on port 8080 of your local machine, you need to pass -p 8080:80 parameter as follows:
    ////////////////////////////////////

    By default a container’s file system persists even after the container exits. This makes debugging a lot easier (since you can inspect the final state) 
    and you retain all your data by default.  you can add the --rm flag to to keep the host clean from stopped and unused containers.
    but it is still available on your disk and you can start it again with : 
    >docker start container-name/ID

    ///////////////////////////////////

    In Dockerfiles, an ENTRYPOINT instruction is used to set executables that will always run when the container is initiated.
    Unlike CMD commands, ENTRYPOINT commands cannot be ignored or overridden—even when the container runs with command line arguments stated.


   /////////////////legacy
   If you specify neither EXPOSE nor -p, the service in the container will only be accessible from inside the container itself.
   If you EXPOSE a port, the service in the container is not accessible from outside Docker, but from inside other Docker containers.
                     (as binds to a vm port, other containers/app on the vm can connect to this port on the vm) 
      The EXPOSE instruction exposes ports for use within links.(legacy). From other containers, you can access all container ports without exposing them. 
      container IP address is unpredictable.link is used to specify which container you want to connect (so you link to specific container IP)
   If you EXPOSE and -p a port, the service in the container is accessible from anywhere, even outside Docker.(as binds container to vm port and vm port to the host port)
   If you do -p, but do not EXPOSE, Docker does an implicit EXPOSE. This is because if a port is open to the public, it is automatically also open to other Docker containers. 
   /////////////////////////

   /////////////////////////////////////
   https://stackoverflow.com/questions/22111060/what-is-the-difference-between-expose-and-publish-in-docker
   choosing a host port depends on the host and hence does not belong to the Dockerfile
   EXPOSE is a way of documenting, related to Dockerfiles ( documenting )
   --publish (or -p) is a way of mapping a host port to a running container port. s related to docker run ... ( execution / run-time )


    You publish ports using the --publish or --publish-all flag to docker run. This tells Docker which ports to open on the container’s network interface. 
    When a port is published, it is mapped to an available high-order port (higher than 30000) on the host machine, 
    unless you specify the port to map to on the host machine at runtime. 
    You cannot specify the port to map to on the host machine when you build the image (in the Dockerfile), 
    because there is no way to guarantee that the port will be available on the host machine where you run the image.
    This creates a firewall rule which maps a container port to a port on the Docker host.
   ///////////////////////////////////// 

   B) instead of docker run, use docker-compose.yml to run many already built images together in their own containers 
     >docker-compose up

    The state of mind when creating a containerized app is not by taking a fresh, clean ubuntu container for instance,
      and downloading the apps and configurations you wish to have in it, and then let it run.
    But creating a custom build/image by using docker build, where you embed all your files, configurations, environment variables etc, into the build/image.
    
    and then run the build/pre configured and downloaded files image using A) >docker run <image> <app> OR B) >docker-compose up


    The key difference between docker run versus docker-compose is that docker run is entirely command line based, 
    while docker-compose reads configuration data from a YAML file. 
    The second major difference is that docker run can only start one container at a time, while docker-compose will configure and run multiple.





8) A docker-compose.yml file to deploy both Docker containers so you can see 
     how the two microservices work together in the development environment (running on my mac locally) 
   To allow the services to talk to each other, we are using Docker Links as a simple "service discovery" mechanism.

    A)rails_frontend:
      image: gruntwork/rails-frontend
      volumes:
        - ./rails-frontend:/usr/src/app
      ports:
        - "3000:3000"
      environment:
        - RAILS_ENV=development
      links:
        - sinatra_backend

    equivalent command in docker run like-
    B)dockerfile build >WORKDIR /usr/src/app
               >COPY . /usr/src/app
     same docker compose doing so that we have fresh code from github everytime volumes: - ./rails-frontend:/usr/src/app    
     
     The docker-compose.yml file mounts rails-frontend and sinatra-backend folders as volumes in each Docker image, 
     so any changes you make to the apps on your host OS will automatically be reflected in the running Docker container.
         
     This lets you do iterative "make-a-change-and-refresh" style development.
     (image will have old code. first mount new code in same path after running server in container)
     (like npx nodemon can change file after running server, server refreshes) 

    C) >docker run -d gruntwork/rails-frontend  -p 3000:3000 -v ./rails-frontend:/usr/src/app   (container port to host/vm port)
      --------------------

        https://iximiuz.com/en/posts/you-need-containers-to-build-an-image/
          Even when you are building your very first image using Docker, podman, or buildah, you are already, albeit implicitly, running containers under the hood!
          images aren't required to run containers. Unlike virtual machines, containers are just isolated and restricted processes on your Linux host. They do form an isolated
           execution environment, including the personalized root filesystem, but the bare minimum to start a container is just a folder with a single executable file inside.
           So, when we are starting a container from an image, the image gets unpacked and its content is provided to the container runtime in a form of a filesystem bundle, 
           i.e. a regular directory containing the future root filesystem files and some configs

          Every time, Docker (or buildah, or podman, etc) encounters a RUN/CMD instruction in the Dockerfile it actually fires a new container! 
          The bundle for this container is formed by the base image plus all the changes made by preceding instructions from the Dockerfile (if any). 
          When the execution of the RUN/CMD step completes, all the changes made to the container's filesystem (so-called diff) become a new layer in 
          the image being built and the process repeats starting from the next Dockerfile instruction. 

          0)alpine image: (built on top of nothing/scrartch)
             FROM scratch
              ADD alpine-minirootfs-3.11.6-x86_64.tar.gz /        //copy from build directory
              CMD ["/bin/sh"] 
             //runs a container from scratch, configues - adds files and runs a bash, whatever input passed to it is run in the bash opened in alpine. and saves the image
             //wget --spider http://dl-cdn.alpinelinux.org/alpine/v3.11/releases/x86_64/alpine-minirootfs-3.11.6-x86_64.tar.gz //x86_64 change to arm64 for m2
          1)sinatra image: (built on top of alpine) 
            FROM alpine:3.6
            ...
            WORKDIR /usr/src/app
            COPY . /usr/src/app
            EXPOSE 4567
            CMD ["ruby", "app.rb"]   
          2) docker compose for service sinatra baclend: (built on top of image: gruntwork/sinatra-backend)
              volumes:
                - ./sinatra-backend:/usr/src/app
              ports:
                - "4567:4567"
              environment:
                - RACK_ENV=development  
             //thus is last preimage run and configured , but now saved in docker compose but keeps running. thus iterative build+run. 
              and then if stop container and store container state(configuration other that build) then like docker start only. >docker-compose start [container id]   

             thus scratch container -> alpine image/container, alpine-> sinatra image/container, sinatra-> docker compose container/built image not saved only run

         3)IAC configuration - packer images also same thing, instead of container, configures and stores and runs vm build and run state.
         4)IAC provision - terraform also same thing, just dont have prebuilt image there but provisioners-functions are  like those only.


    A)>docker build ; building images is just an recursive process by running a previous image/built and then configuring it and saving it. 
    thus build runs the prev image first in a container and then makes changes to it and saves.

    B)>docker run <image> <app>; just runs/restarts the image/build again in a new container
      >docker start <container> ; just restarts the container(saved vm/container/image state when confguration change due to interaction with app also there)
                                                (not only configuration change due to the build) /LINKS 
                                               (VM to HOST like PORTS/-p in compose/RUN not in build as no way to guarantee the port will be available on host)
                 and few configure both like VOLUMES(actually configures vm to container only while compose/RUN/BUILD, 
                 the host to vm done using >colima start --mount $HOME:w --mount /Volumes/Data:w),
                  build and keep running after that) 
              just difference it that it is not saving the build just keep on running like docker run.
              thus like docker build iteratively  + docker run together = docker compose.     
                 and same container state also stores due to configuration due to interaction >docker start, thus like docker start also.
              just one difference is can dont need to store build first before run, both at same time. and can support multiple container and config in a single file.   



9) Terraform(for prod(non local-dev) setup) configurations to deploy both Docker containers on Amazon's EC2 Container Service (ECS) (terraform runs docker-compose.yml in ec2) 
      so you can see how the two microservices work together in the production environment.
   To allow the services to talk to each other, we deploy an Elastic Load Balancer (ELB) in front of each service 
      and use Terraform to pass the ELB URLs between services. We are using the same environment variables as Docker Links, 
      so this acts as a simple "service discovery" mechanism that works in both dev and prod.    

10) Sign up for an AWS account. Configure your AWS credentials as environment variables for terraform:
    export AWS_ACCESS_KEY_ID=...
    export AWS_SECRET_ACCESS_KEY=...
11)Install Terraform. 
12)cd terraform-configurations. Open vars.tf, set the environment variables specified at the top of the file
13) terraform init, terraform plan, terraform apply to deploy the code into your AWS account.
    You can monitor the state of the ECS cluster using the ECS Console.
    After terraform apply completes, it will output the URLs of the ELBs of the rails-frontend and sinatra-backend apps.

14) Deploying new versions
    Every time you want to deploy a new version of one of the microservices, you need to:
    14.1)Build a new version of the Docker image for that microservices
    (or better yet, create a CI job that builds a new image after every commit). 
    14.2)go to point 7 to change docker-compose.yml and terraform-configurations/terraform.tfvars
     14.3)goto point 12,13. 


-------------------------------------------------------------------------------------------------------------------------











-------------------------------------------------------------------------------------------------------------------------



DOCKER-

dockerfile-

FROM rails:4.2.6   //base image, [platform] arg can pass if needed , installs based on the built platform in linux, window etc. mine is mac m2 apple silion 
LABEL maintainer "Gruntwork <info@gruntwork.io>"

ARG <name>[=<default value>] // a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.

# Source code should be in the /usr/src/app folder
WORKDIR /usr/src/app     //workdir of the app inside the container os

# Install dependencies in a cache-friendly way  //copy the package.json and do npm install first and then copy rest code
COPY Gemfile Gemfile.lock /usr/src/app/
RUN bundle install                              //means > /bin/sh -c bundle install    

# Now copy the rest of the source code
COPY . /usr/src/app
RUN chmod +x /usr/src/app/docker-entrypoint.sh

# Run the rails server by default
EXPOSE 3000
ENTRYPOINT ["bash","/usr/src/app/docker-entrypoint.sh"]
CMD ["rails", "server", "-b", "0.0.0.0"]

---------------------------------------------------------------

//////////////////////////
>brew install docker  //path = /opt/homebrew/Cellar/docker/27.0.3
>sudo systemctl start docker    //>docker ps    //error - Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
 In multitasking computer operating systems, a daemon (/ˈdiːmən/ or /ˈdeɪmən/)[1] is a computer program that runs as a background process, 
 rather than being under the direct control of an interactive user.
 >sudo systemctl enable docker

>brew install homebrew/cask/docker  //install docker desktop on mac as cant easily start docker daemon in mac 
//Linking Binary 'docker' to '/usr/local/bin/docker'
//Homebrew-Cask extends Homebrew and allows you to install large binary files via a command-line tool. //appstore for opensource code packages and binary apps

>open -a Docker // start the docker desktop itself in macos :
>killall Docker //stop the docker desktop itself in macos :
//////////////////////////////

----------------------------------------------------------------------------------------------------------------------------------

////////////////////////////////////////////////////////////////////////////////////////////////////////////////

http_proxy is docker. configure http_proxy and furtherforward header 
     with my application gateway(zscalar router public ip) in postman(as reverse web to my host), stripe(as reverse web to my host)                                     
            and dockerfile(virtualization os/container doesnt know about my proxy  (my host to web and reverse)).  
            thus it send data to docker.com from container, uses the host/vm network , 
             gets back zscalar signed certificate for docker.com from proxy zscalar and was expectiong rootCA of docker.com signed certificate


move zscalar root ca in cert
1)from scratch
move cert
from scratch
move cert
configure http proxy in docker. or install ca-ceritificate first(without alpine can do?) in dokcer container and build fron scratch (but cant add alpine later)

OR, the network settings must be of docker runtime celine and inside that docker daemon/server dockerd
mover cert to celine/docker d

docker run -v /etc/ssl/certs:/etc/ssl/certs
Does the server you are connecting expect client certificate?
 
 openssl s_client -connect registry-1.docker.io:443 -showcerts </dev/null

If Yes, then --cert /data/scripts/mms/server.crt should be client certificate and --key /data/scripts/mms/server.key should be client key along with --cacert <server root CA certificate> (this can be suppressed by passing -k option). 
Corresponding Client certificate chain or Root cert should be present on the server trust store.

so basically our host vm runs celine runtime which runs docekrd server(just a server thus should use my host settings no) which runs container.
so mostly need to change network settings of dokcer container


https://docs.docker.com/desktop/faqs/macfaqs/#how-do-i-add-tls-certificates
https://stackoverflow.com/questions/70086858/how-to-specify-and-use-a-cert-file-during-a-docker-build
https://stackoverflow.com/questions/70086858/how-to-specify-and-use-a-cert-file-during-a-docker-build




These settings are used to configure proxy environment variables for containers only, and not used as proxy settings for the Docker CLI or the Docker Engine itself
When you start a container, its proxy-related environment variable are set to reflect your proxy configuration in ~/.docker/config.json
When you invoke a build, proxy-related build arguments are pre-populated automatically, based on the proxy settings in your Docker client configuration file.
{
 "proxies": {
   "default": {
     "httpProxy": "http://proxy.example.com:3128",
     "httpsProxy": "https://proxy.example.com:3129",
     "noProxy": "*.test.example.com,.example.org,127.0.0.0/8"
   }
 }
}

or create env variables in containers
export HTTP_PROXY=127.0.0.1:9000 - to bypass zscalar only for the particular terminal session. 
                                      for permanent add to .zshenv/.zshrc but dont do as need to connect to servicenow
>export HTTPS_PROXY=127.0.0.1:9000
>echo $HTTPS_PROXY 
>unset HTTPS_PROXY
and then unset them later


but my host localhost:9000 not same as the container localhost:9000
thus use 10.160.68.155:9000 but they are not listening to it


////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////



>systemctl start docker //starts the docker runtime and server daemon dockerd. //this docker different from docker client
>colima start           //starts the docker runtime and server daemon dockerd.
docker desktop and colima/rancher just run docker daemon/server and just act as frontendGUI/management tool to check daemon and runtime state etc.
they dont have control over docker client as docker client is in layer above them(runtime level 2).(runtime level 1 is runc/contrad)



>ps aux, >top, >systemctl list-units --type=service --state=running
The equivalent to Linux >systemctl on macOS would be >launchctl
>ls /usr/local/bin      //shortcut path


https://news.ycombinator.com/item?id=33769786
https://medium.com/@senali/what-is-the-difference-between-docker-runtime-and-docker-engine-235b25ae9a73#:~:text=It%20provides%20the%20runtime%20environment,containers%20on%20a%20host%20machine.
https://github.com/abiosoft/colima
Docker daemon (dockerd). It is a program that manages all the Docker objects, including images, containers, volumes and many more.
Another entity, the Docker client(frontend), helps pass the command from the user to the Docker service(runtime) via the Docker daemon(server).
Under certain circumstances, the Docker client fails to connect to the Docker daemon. The Docker throws the error Cannot connect to Docker daemon in such a case.
There could be various reasons why a Docker client cannot connect to the Docker daemon.

instead of docker desktop manager, use colima manager to run docker
Docker Engine comprises both the Docker Daemon (dockerd)(server) and the Docker CLI (docker)(client) and a docker runtime. 
The Docker Runtime, as a subset of the Docker Engine, is responsible for actually running containers.
 The default runtime historically used by Docker is "runc," but users can opt for alternative runtimes like containerd if they choose. 
 This modular architecture provides flexibility and compatibility within the container ecosystem.

Does Colima work with Docker Compose?
Compose is a layer on top of the Docker client/API(frontend), so the container runtime provider level 2(colima uses default docker runc runtime level1) shouldn't matter, 
I guess as long as there's a real dockerd(server) under the hood, and modulo any bugs. colima by default uses dockerd(server) and docker runc

docker plugin for VScode is just the docker client that listen to the docker runtime/docker daemon/server. just docker client fns they 
in turn may be able to configure docker daemon like >docker build, >docker run etc. or >docker ps etc.


IMP
   1) docker server ways to start (colima is cli extraction and docker desktop is gui extraction over docker server runtime/daemon) vm
    >colima start(cli), >docker desktop(gui), >homebrew cask(install docker desktop gui through cli)  are just runtimes to start docker daemon/server (runc inside) 
    through cli >colima start or through gui dockerdesktop or through cli >systemctl start docker
       ------------------------- starts the vm with config from host
   
   2) docker cli is just frontend >brew install docker docker-compose.  this can be installed and used independently via cli brew or 
         instead of docker cli used docker desktop for frontend also eg looking at all docker containers instead of >docker ps etc

        ------------------------- talk to the vm and containers from host.      


>brew install docker    //install the client
>brew install docker-compose   //no need to separately install docker -compase as comes with docker ,  >docker compose up
                        //install the wrapper api above/for docker client
//>chmod +x /usr/local/bin/docker-compose dont need
>brew install colima    //install the runtime(comes with dockerd(server))
>cp proxy-cert-2.pem ~/.docker/certs.d/   //https://passingcuriosity.com/2023/colima-docker-on-mac/
>colima start           //runs the runtime(colima) and the server(dockerd)
docker daemon and runtime started, now can run all docker commands
>colima stop

>colima start --with kubernetes 
-----
>docker --version 
>docker-compose --version
>docker ps
>docker logs --tail 100 <container ID>
>docker image ls
>sudo usermod -a -G docker <username>
//>docker-compose --version
> docker compose up --build
>docker-compose up  -d   //in background

docker ps lists all running containers in docker engine. docker-compose ps lists containers related to images declared in docker-compose file
//>/opt/homebrew/bin/colima daemon start default

>docker login
Log in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one.
You can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO.
>docker network prune
>docker pull (from registry default or someplace else)

>docker build -t rails-frontend-local-image . 







try terraform, terraform should work as it is using aws vms to docker compose and run only thus from my host can connect to aws 
and aws connect to dockerhub

A)vm and server
Colima/docker desktop docker d itself runs a vm of 2gb ram 60gb ssd. the vm then runs other containers within it while building and running.
>colima start and >systemctl start docker  //starts the vm which is  dockerd which is the docker server


B)network
The bind port binds the container port to vm port. (In expose)
And in -p flag in >docker run or PORT in >docker-compose; binds vm port to the host.
And inside vm, each container can talk to other container without config as running in different ports in vm like localhost



C) File from host to vm to container
  host to vm
    Colima only mounts the $HOME directory by default from host to vm, so your data is not being preserved.
    If you must use a directory outside of $HOME, you need to override the default mounts.
    >colima start --mount $HOME:w --mount /Volumes/Data:w

    FROM scratch
    ADD alpine-minirootfs-3.11.6-x86_64.tar.gz /
    CMD ["/bin/sh"]
    
  vm to container 
    in dockerfile
      WORKDIR /usr/src/app
      COPY . /usr/src/app
    in docker-compose 
      volumes:
      - ./rails-frontend:/usr/src/app
       

0) my client cert also add along with cacert cp Certificates2.cer ~/.docker/certs.d/registry-1.docker.io/
1) need to add cert to Colima
>cp proxy-cert-2.pem ~/.docker/certs.d/      //solves the problem
2) need to add cert to every image (and then on gloal hub check if image exists then add) (hassle)
https://stackoverflow.com/questions/70086858/how-to-specify-and-use-a-cert-file-during-a-docker-build
COPY ./trusted-certs.pem /usr/local/share/ca-certificates/            //copy .pem from vm to container
RUN cat /usr/local/share/ca-certificates/trusted-certs.pem >> /etc/ssl/certs/ca-certificates.crt  //convert to .crt in continer 

# COPY ./trusted-certs.pem /usr/local/share/ca-certificates/    //copy between host/vm and contiainer
# # RUN cat /usr/local/share/ca-certificates/trusted-certs.pem >> /etc/ssl/certs/ca-certificates.crt
# RUN cp /usr/local/share/ca-certificates/trusted-certs.pem  /etc/ssl/certs/   //copy inside container 
# COPY ./proxy-cert.cer /usr/local/share/ca-certificates/  
# RUN cp /usr/local/share/ca-certificates/proxy-cert.cer  /etc/ssl/certs/
# Source code should be in the /usr/src/app folder

COPY trusted-certs.pem /root/trusted-certs.pem
RUN SSL_CERT_FILE=~/trusted-certs.pem apk add ca-certificates
RUN update-ca-certificates

use SSL_CERT_DIRECTORY and add all the certificates there
need to add at depth 2 and depth 3 very big hassle

3) but i cant even run  other images on my mac as even those containers build needs proxy certs

https://hub.docker.com/repository/docker/bhattacharjeeabhinav/ecommerceapp/general

----------------------------------------------------------------------------------------------------------------------------------

















----------------------------------------------------------------------------------------------------------------------------------


just to build to image use my local computer and run them,  terraform aws part try to do on mac( work as docker registry image able to access using gcp vms),
docker tutorial part read, finish the course on personal laptop 


1)(e.g., Podman, Buildah, aws lambda)  - docker (server or client?) alternative
2)colima cli for mac/linux(server) , Rancher Desktop gui for mac,linux,windows(server+client), systemctl linux(server) 
                                              -  alternative for docker desktop gui(server+client) by Docker, Inc.
                                                        (docker desktop is paid, but the runtime dockerd/runc is free and colima uses that opensource)

3)While Docker is a container runtime, Kubernetes is a platform for running and managing containers from many container runtimes(many vms like 5 linux 2 windows).
  Kubernetes supports numerous container runtimes including Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI (Container Runtime Interface).
    written in GO by google, now maintianed opensource 



>docker push bhattacharjeeabhinav/ecommerceapp:tagname



OS/arch
  Docker has been able to run Linux containers on Windows desktop since it was first released in 2016 (before Hyper-V isolation or Linux containers on Windows were available) 
  using a LinuxKit based virtual machine running on Hyper-V(windows)
  //Microsoft Hyper-V known before its release as Windows Server Virtualization, is a native hypervisor; it can create virtual machines on x86-64 systems running Windows

  mac is arm64(risc), linux amd64(extension of x86 cisc by amd company), windows x86-64(cisc by intel company)
  windows container on mac -> use a separate vm in between that is able to run windows vm/apps on mac like virtualbox/vagrant
  mac container on windows -> uses a separate vm in between like hyper-v. 
  both client and server built containers atleast need to be both windows/mac/linux so that they are run as separate containers 
    on a single docker vm(abstracts oS/arch) on any OS/arch.
  https://www.geekboots.com/story/arm-vs-x86-vs-amd64


   Q) run both windows and linux containers on windows host
   ans) If you need Linux containers and Windows containers you can use Docker Desktop to run Windows containers and 
        WSL to run Linux containers and forward the necessary ports to the WSL host from the Windows host.
        
        windows -> docker desktop/colima, dockerd/runc daemon vm -> windows container  (one vm needed, one container)
        windows ---port forwarding--> wsl(vm virtualization linux for windows) --> docker desktop/colima, dockerd/runc daemon vm -> linux container (2 vm needed, one container)
          LIKE running 2 docker vms in a kubernetes docker swarm OR like two ec2 instances in a ecs docker instance.
       
        https://forums.docker.com/t/working-with-windows-and-linux-containers-in-docker-compose/134318


 by default the docker server vm is linux only. thus in windows machine uses another basevm wsl and in that uses the actual docker vm thus 2 vms by default like.
    thus starting and stopping docker daemon takes time in windows

 i dont need to but using multiplatform image ability in my containerd vm windows. thus using docker-linux for amd64 builder 


    Play with Docker uses the amd64 platform. If you are using an ARM based Mac with Apple silicon, you will need to rebuild the image to be compatible with Play with Docker and push the new image to your repository.
    To build an image for the amd64 platform, use the --platform flag.
    >docker build --platform linux/amd64 -t YOUR-USER-NAME/getting-started .
    Docker buildx also supports building multi-platform images. To learn more, see Multi-platform images.

  IMP - thus not only docker images, all the executables/application executables ask for os/arch as the executable machine level binary machine code depends on the arch. 

---------------------
instead of links, use depends_on: server in docker compose


>docker build -t rails-frontend-image .   //rails-frontend-image:latest is the repository:tag, uses latest by default,  . is the path to dockerfile
>docker image ls

>docker tag 7282248e289c bhattacharjeeabhinav/ecommerceapp:rails-frontend-image     //use imageid or oldlocalimagerepo:tag both works
REPOSITORY                          TAG                    IMAGE ID       CREATED          SIZE
bhattacharjeeabhinav/ecommerceapp   rails-frontend-image   7282248e289c   12 minutes ago   1.54GB
rails-frontend-image                latest                 7282248e289c   12 minutes ago   1.54GB
>docker login
>docker push bhattacharjeeabhinav/ecommerceapp:rails-frontend-image       //push the image with this repository:tag to our loggedin account same repo with same tag
>docker pull bhattacharjeeabhinav/ecommerceapp:rails-frontend-image

the localimage 2.9GB is saved somewhere not is home directory
tag image, login to dockerhub, push image. 
//same with github, pulling, making changes locally doesnt need login(till now doesnt have identity) but pushing need login as need to know who is pushing. 
     //(checks in the collaborators section for push and pull section)
     //dockerhub also works like github only

 //>docker history 7282248e289c

1)You can push the docker image to the Dockerhub with using docker push command aftering changing the tag of the image with your docker hub username and repository name.
2)push to dockerhub using jenkinsfile -   
  You can push the docker image to the docker hub using the Jenkinsfile by specifying the steps such as docker build , push of the docker image using the Docker Pipeline plugins.
    Q)is jenkinsfile like a bash script? Ans) mostly yes, .yaml like .sh line by line script
3)Q)How do I push a docker image from GitHub to Docker Hub?
  Ans) We can push the docker image from Github to the Dockerhub by configuring the github actions workflow CI/CD
               (thus github actions also is CI/CD shell online like jenkins,  just a shell script .yaml instead of .sh) 
       OR by configuring any other CI/CD pipeline we are using (like jenkins instead of github actions)
       that builds the docker image from the github repository and pushes it to the Dockerhub.  

https://github.com/docker/awesome-compose/blob/master/react-express-mongodb/compose.yaml



    image: bhattacharjeeabhinav/ecommerceapp:rails-frontend-image 
>docker-compose up   //recreates/builds the container and runs 
OR >docker run bhattacharjeeabhinav/ecommerceapp:rails-frontend-image   //image   
//docker run -dp 0.0.0.0:3000:3000 YOUR-USER-NAME/getting-started

>docker-compose ps
>docker-compose images  //the docker compose is build only actually and when stop just stops the container

>docker stop  9d735c0f79dc6f62eb30d4439ef400e0fe183f855744900b0b29c46feb836c64 //any specific container stop
           //but doesnt delete the built new image
>docker-compose stop   
           //stops all the services or any service you mentioned and their containers, but doesnt delete the build new/fetched image
>docker-compose start 
           //starts/runs/resumes  all the images/services or any service you mentioned //doesnt create again/build, to create again/build use docker-compose up

there is difference between images and containers.   
when building/run/docker-compose up, images are built/fetched and stored. and then again NEW containers are run using them as base
when we stop or remove containers,(like new images) only these containers are stopped/removed not the image itself


>docker ps -a    //show all containers including stopped/exited
>docker rm -f 7282248e289c   //containerid
          //to remove all containers(need to build again/pull from repo using docker run/ docker-compose up if already image not there in local)
          (if a container is running, then we cant run the container again from same image on same vm port. we need to stop the container(so that the vm port gets free
           but better is to stop and remove the container))
>docker-compose rm  //remove all services and their containers


In this section, you learned how to share your images by pushing them to a registry. 
You then went to a brand new instance and were able to run the freshly pushed image. This is quite common in CI pipelines, 
where the pipeline will create the image and push it to a registry and then the production environment can use the latest version of the image.


>docker volume create todo-db
>docker run -dp 127.0.0.1:3000:3000 --mount type=volume,src=todo-db,target=/etc/todos getting-started
>docker volume inspect todo-db

https://docs.docker.com/guides/workshop/04_sharing_app/



services:
  frontend:
    build:
      context: frontend
      target: development
    ports:
      - 3000:3000
         //container application connect and listen on this vm port and host port forwarded to vm port so that public internet traffic also connect
         //127.0.0.1:3000:3000    host interface:port to vm port(container implicit)
    stdin_open: true
    volumes:
      - ./frontend:/usr/src/app
      - /usr/src/app/node_modules
    restart: always
    networks:
      - react-express
    depends_on:
      - backend

  backend:
    restart: always
    build:
      context: backend
      target: development
    volumes:
      - ./backend:/usr/src/app
      - /usr/src/app/node_modules
            //vm default directory to container directory data flows
    depends_on:
      - mongo
    networks:
      - express-mongo
      - react-express
    expose: 
      - 3000
  mongo:
    restart: always
    image: mongo:4.2.0
    volumes:
      - mongo_data:/data/db           
         //container taking data from the docker vm created volume on vm, instead of the vm default directory (which in turn taken from host but that not confogured here)
         //the volume is loaded as file system on the container
    networks:
      - express-mongo
         //internal container network etc firewall must be open for this service
    expose:
      - 27017
         //just container application connect and listen on this vm port, so that containers can talk to other containers in the vm
networks:               ///networks is like namespace, services subscribed to this tag can talk to all the other services in that network, no firewall etc.
  react-express:
  express-mongo:

volumes:
  mongo_data: 
    //if local db instead of cloud atlas then use volume to store data even if service/containers get stopped and removed.
    // (as long as vm is there, this volume/data is there on the vm's disk)


https://github.com/docker/awesome-compose/blob/master/react-express-mongodb/compose.yaml
----------------------------------------------------------------------------------------------------------------------------------------





















